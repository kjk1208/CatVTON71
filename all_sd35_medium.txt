# SD3.5 dump for 'stabilityai/stable-diffusion-3.5-medium'  (20250916_152928)

=== MODULE TREE (filtered) ===
transformer :: SD3Transformer2DModel
transformer.pos_embed :: PatchEmbed
transformer.pos_embed.proj :: Conv2d
transformer.time_text_embed :: CombinedTimestepTextProjEmbeddings
transformer.time_text_embed.time_proj :: Timesteps
transformer.time_text_embed.timestep_embedder :: TimestepEmbedding
transformer.time_text_embed.timestep_embedder.linear_1 :: Linear
transformer.time_text_embed.timestep_embedder.act :: SiLU
transformer.time_text_embed.timestep_embedder.linear_2 :: Linear
transformer.time_text_embed.text_embedder :: PixArtAlphaTextProjection
transformer.time_text_embed.text_embedder.linear_1 :: Linear
transformer.time_text_embed.text_embedder.act_1 :: SiLU
transformer.time_text_embed.text_embedder.linear_2 :: Linear
transformer.context_embedder :: Linear
transformer.transformer_blocks :: ModuleList
transformer.transformer_blocks.0 :: JointTransformerBlock
transformer.transformer_blocks.0.norm1 :: SD35AdaLayerNormZeroX
transformer.transformer_blocks.0.norm1.silu :: SiLU
transformer.transformer_blocks.0.norm1.linear :: Linear
transformer.transformer_blocks.0.norm1.norm :: LayerNorm
transformer.transformer_blocks.0.norm1_context :: AdaLayerNormZero
transformer.transformer_blocks.0.norm1_context.silu :: SiLU
transformer.transformer_blocks.0.norm1_context.linear :: Linear
transformer.transformer_blocks.0.norm1_context.norm :: LayerNorm
transformer.transformer_blocks.0.attn :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.0.attn.norm_q :: RMSNorm
transformer.transformer_blocks.0.attn.norm_k :: RMSNorm
transformer.transformer_blocks.0.attn.to_q :: Linear
transformer.transformer_blocks.0.attn.to_k :: Linear
transformer.transformer_blocks.0.attn.to_v :: Linear
transformer.transformer_blocks.0.attn.add_k_proj :: Linear
transformer.transformer_blocks.0.attn.add_v_proj :: Linear
transformer.transformer_blocks.0.attn.add_q_proj :: Linear
transformer.transformer_blocks.0.attn.to_out :: ModuleList
transformer.transformer_blocks.0.attn.to_out.0 :: Linear
transformer.transformer_blocks.0.attn.to_out.1 :: Dropout
transformer.transformer_blocks.0.attn.to_add_out :: Linear
transformer.transformer_blocks.0.attn.norm_added_q :: RMSNorm
transformer.transformer_blocks.0.attn.norm_added_k :: RMSNorm
transformer.transformer_blocks.0.attn2 :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.0.attn2.norm_q :: RMSNorm [CROSS]
transformer.transformer_blocks.0.attn2.norm_k :: RMSNorm [CROSS]
transformer.transformer_blocks.0.attn2.to_q :: Linear [CROSS]
transformer.transformer_blocks.0.attn2.to_k :: Linear [CROSS]
transformer.transformer_blocks.0.attn2.to_v :: Linear [CROSS]
transformer.transformer_blocks.0.attn2.to_out :: ModuleList [CROSS]
transformer.transformer_blocks.0.attn2.to_out.0 :: Linear [CROSS]
transformer.transformer_blocks.0.attn2.to_out.1 :: Dropout [CROSS]
transformer.transformer_blocks.0.norm2 :: LayerNorm
transformer.transformer_blocks.0.ff :: FeedForward
transformer.transformer_blocks.0.ff.net :: ModuleList
transformer.transformer_blocks.0.ff.net.0 :: GELU
transformer.transformer_blocks.0.ff.net.0.proj :: Linear
transformer.transformer_blocks.0.ff.net.1 :: Dropout
transformer.transformer_blocks.0.ff.net.2 :: Linear
transformer.transformer_blocks.0.norm2_context :: LayerNorm
transformer.transformer_blocks.0.ff_context :: FeedForward
transformer.transformer_blocks.0.ff_context.net :: ModuleList
transformer.transformer_blocks.0.ff_context.net.0 :: GELU
transformer.transformer_blocks.0.ff_context.net.0.proj :: Linear
transformer.transformer_blocks.0.ff_context.net.1 :: Dropout
transformer.transformer_blocks.0.ff_context.net.2 :: Linear
transformer.transformer_blocks.1 :: JointTransformerBlock
transformer.transformer_blocks.1.norm1 :: SD35AdaLayerNormZeroX
transformer.transformer_blocks.1.norm1.silu :: SiLU
transformer.transformer_blocks.1.norm1.linear :: Linear
transformer.transformer_blocks.1.norm1.norm :: LayerNorm
transformer.transformer_blocks.1.norm1_context :: AdaLayerNormZero
transformer.transformer_blocks.1.norm1_context.silu :: SiLU
transformer.transformer_blocks.1.norm1_context.linear :: Linear
transformer.transformer_blocks.1.norm1_context.norm :: LayerNorm
transformer.transformer_blocks.1.attn :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.1.attn.norm_q :: RMSNorm
transformer.transformer_blocks.1.attn.norm_k :: RMSNorm
transformer.transformer_blocks.1.attn.to_q :: Linear
transformer.transformer_blocks.1.attn.to_k :: Linear
transformer.transformer_blocks.1.attn.to_v :: Linear
transformer.transformer_blocks.1.attn.add_k_proj :: Linear
transformer.transformer_blocks.1.attn.add_v_proj :: Linear
transformer.transformer_blocks.1.attn.add_q_proj :: Linear
transformer.transformer_blocks.1.attn.to_out :: ModuleList
transformer.transformer_blocks.1.attn.to_out.0 :: Linear
transformer.transformer_blocks.1.attn.to_out.1 :: Dropout
transformer.transformer_blocks.1.attn.to_add_out :: Linear
transformer.transformer_blocks.1.attn.norm_added_q :: RMSNorm
transformer.transformer_blocks.1.attn.norm_added_k :: RMSNorm
transformer.transformer_blocks.1.attn2 :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.1.attn2.norm_q :: RMSNorm [CROSS]
transformer.transformer_blocks.1.attn2.norm_k :: RMSNorm [CROSS]
transformer.transformer_blocks.1.attn2.to_q :: Linear [CROSS]
transformer.transformer_blocks.1.attn2.to_k :: Linear [CROSS]
transformer.transformer_blocks.1.attn2.to_v :: Linear [CROSS]
transformer.transformer_blocks.1.attn2.to_out :: ModuleList [CROSS]
transformer.transformer_blocks.1.attn2.to_out.0 :: Linear [CROSS]
transformer.transformer_blocks.1.attn2.to_out.1 :: Dropout [CROSS]
transformer.transformer_blocks.1.norm2 :: LayerNorm
transformer.transformer_blocks.1.ff :: FeedForward
transformer.transformer_blocks.1.ff.net :: ModuleList
transformer.transformer_blocks.1.ff.net.0 :: GELU
transformer.transformer_blocks.1.ff.net.0.proj :: Linear
transformer.transformer_blocks.1.ff.net.1 :: Dropout
transformer.transformer_blocks.1.ff.net.2 :: Linear
transformer.transformer_blocks.1.norm2_context :: LayerNorm
transformer.transformer_blocks.1.ff_context :: FeedForward
transformer.transformer_blocks.1.ff_context.net :: ModuleList
transformer.transformer_blocks.1.ff_context.net.0 :: GELU
transformer.transformer_blocks.1.ff_context.net.0.proj :: Linear
transformer.transformer_blocks.1.ff_context.net.1 :: Dropout
transformer.transformer_blocks.1.ff_context.net.2 :: Linear
transformer.transformer_blocks.2 :: JointTransformerBlock
transformer.transformer_blocks.2.norm1 :: SD35AdaLayerNormZeroX
transformer.transformer_blocks.2.norm1.silu :: SiLU
transformer.transformer_blocks.2.norm1.linear :: Linear
transformer.transformer_blocks.2.norm1.norm :: LayerNorm
transformer.transformer_blocks.2.norm1_context :: AdaLayerNormZero
transformer.transformer_blocks.2.norm1_context.silu :: SiLU
transformer.transformer_blocks.2.norm1_context.linear :: Linear
transformer.transformer_blocks.2.norm1_context.norm :: LayerNorm
transformer.transformer_blocks.2.attn :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.2.attn.norm_q :: RMSNorm
transformer.transformer_blocks.2.attn.norm_k :: RMSNorm
transformer.transformer_blocks.2.attn.to_q :: Linear
transformer.transformer_blocks.2.attn.to_k :: Linear
transformer.transformer_blocks.2.attn.to_v :: Linear
transformer.transformer_blocks.2.attn.add_k_proj :: Linear
transformer.transformer_blocks.2.attn.add_v_proj :: Linear
transformer.transformer_blocks.2.attn.add_q_proj :: Linear
transformer.transformer_blocks.2.attn.to_out :: ModuleList
transformer.transformer_blocks.2.attn.to_out.0 :: Linear
transformer.transformer_blocks.2.attn.to_out.1 :: Dropout
transformer.transformer_blocks.2.attn.to_add_out :: Linear
transformer.transformer_blocks.2.attn.norm_added_q :: RMSNorm
transformer.transformer_blocks.2.attn.norm_added_k :: RMSNorm
transformer.transformer_blocks.2.attn2 :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.2.attn2.norm_q :: RMSNorm [CROSS]
transformer.transformer_blocks.2.attn2.norm_k :: RMSNorm [CROSS]
transformer.transformer_blocks.2.attn2.to_q :: Linear [CROSS]
transformer.transformer_blocks.2.attn2.to_k :: Linear [CROSS]
transformer.transformer_blocks.2.attn2.to_v :: Linear [CROSS]
transformer.transformer_blocks.2.attn2.to_out :: ModuleList [CROSS]
transformer.transformer_blocks.2.attn2.to_out.0 :: Linear [CROSS]
transformer.transformer_blocks.2.attn2.to_out.1 :: Dropout [CROSS]
transformer.transformer_blocks.2.norm2 :: LayerNorm
transformer.transformer_blocks.2.ff :: FeedForward
transformer.transformer_blocks.2.ff.net :: ModuleList
transformer.transformer_blocks.2.ff.net.0 :: GELU
transformer.transformer_blocks.2.ff.net.0.proj :: Linear
transformer.transformer_blocks.2.ff.net.1 :: Dropout
transformer.transformer_blocks.2.ff.net.2 :: Linear
transformer.transformer_blocks.2.norm2_context :: LayerNorm
transformer.transformer_blocks.2.ff_context :: FeedForward
transformer.transformer_blocks.2.ff_context.net :: ModuleList
transformer.transformer_blocks.2.ff_context.net.0 :: GELU
transformer.transformer_blocks.2.ff_context.net.0.proj :: Linear
transformer.transformer_blocks.2.ff_context.net.1 :: Dropout
transformer.transformer_blocks.2.ff_context.net.2 :: Linear
transformer.transformer_blocks.3 :: JointTransformerBlock
transformer.transformer_blocks.3.norm1 :: SD35AdaLayerNormZeroX
transformer.transformer_blocks.3.norm1.silu :: SiLU
transformer.transformer_blocks.3.norm1.linear :: Linear
transformer.transformer_blocks.3.norm1.norm :: LayerNorm
transformer.transformer_blocks.3.norm1_context :: AdaLayerNormZero
transformer.transformer_blocks.3.norm1_context.silu :: SiLU
transformer.transformer_blocks.3.norm1_context.linear :: Linear
transformer.transformer_blocks.3.norm1_context.norm :: LayerNorm
transformer.transformer_blocks.3.attn :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.3.attn.norm_q :: RMSNorm
transformer.transformer_blocks.3.attn.norm_k :: RMSNorm
transformer.transformer_blocks.3.attn.to_q :: Linear
transformer.transformer_blocks.3.attn.to_k :: Linear
transformer.transformer_blocks.3.attn.to_v :: Linear
transformer.transformer_blocks.3.attn.add_k_proj :: Linear
transformer.transformer_blocks.3.attn.add_v_proj :: Linear
transformer.transformer_blocks.3.attn.add_q_proj :: Linear
transformer.transformer_blocks.3.attn.to_out :: ModuleList
transformer.transformer_blocks.3.attn.to_out.0 :: Linear
transformer.transformer_blocks.3.attn.to_out.1 :: Dropout
transformer.transformer_blocks.3.attn.to_add_out :: Linear
transformer.transformer_blocks.3.attn.norm_added_q :: RMSNorm
transformer.transformer_blocks.3.attn.norm_added_k :: RMSNorm
transformer.transformer_blocks.3.attn2 :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.3.attn2.norm_q :: RMSNorm [CROSS]
transformer.transformer_blocks.3.attn2.norm_k :: RMSNorm [CROSS]
transformer.transformer_blocks.3.attn2.to_q :: Linear [CROSS]
transformer.transformer_blocks.3.attn2.to_k :: Linear [CROSS]
transformer.transformer_blocks.3.attn2.to_v :: Linear [CROSS]
transformer.transformer_blocks.3.attn2.to_out :: ModuleList [CROSS]
transformer.transformer_blocks.3.attn2.to_out.0 :: Linear [CROSS]
transformer.transformer_blocks.3.attn2.to_out.1 :: Dropout [CROSS]
transformer.transformer_blocks.3.norm2 :: LayerNorm
transformer.transformer_blocks.3.ff :: FeedForward
transformer.transformer_blocks.3.ff.net :: ModuleList
transformer.transformer_blocks.3.ff.net.0 :: GELU
transformer.transformer_blocks.3.ff.net.0.proj :: Linear
transformer.transformer_blocks.3.ff.net.1 :: Dropout
transformer.transformer_blocks.3.ff.net.2 :: Linear
transformer.transformer_blocks.3.norm2_context :: LayerNorm
transformer.transformer_blocks.3.ff_context :: FeedForward
transformer.transformer_blocks.3.ff_context.net :: ModuleList
transformer.transformer_blocks.3.ff_context.net.0 :: GELU
transformer.transformer_blocks.3.ff_context.net.0.proj :: Linear
transformer.transformer_blocks.3.ff_context.net.1 :: Dropout
transformer.transformer_blocks.3.ff_context.net.2 :: Linear
transformer.transformer_blocks.4 :: JointTransformerBlock
transformer.transformer_blocks.4.norm1 :: SD35AdaLayerNormZeroX
transformer.transformer_blocks.4.norm1.silu :: SiLU
transformer.transformer_blocks.4.norm1.linear :: Linear
transformer.transformer_blocks.4.norm1.norm :: LayerNorm
transformer.transformer_blocks.4.norm1_context :: AdaLayerNormZero
transformer.transformer_blocks.4.norm1_context.silu :: SiLU
transformer.transformer_blocks.4.norm1_context.linear :: Linear
transformer.transformer_blocks.4.norm1_context.norm :: LayerNorm
transformer.transformer_blocks.4.attn :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.4.attn.norm_q :: RMSNorm
transformer.transformer_blocks.4.attn.norm_k :: RMSNorm
transformer.transformer_blocks.4.attn.to_q :: Linear
transformer.transformer_blocks.4.attn.to_k :: Linear
transformer.transformer_blocks.4.attn.to_v :: Linear
transformer.transformer_blocks.4.attn.add_k_proj :: Linear
transformer.transformer_blocks.4.attn.add_v_proj :: Linear
transformer.transformer_blocks.4.attn.add_q_proj :: Linear
transformer.transformer_blocks.4.attn.to_out :: ModuleList
transformer.transformer_blocks.4.attn.to_out.0 :: Linear
transformer.transformer_blocks.4.attn.to_out.1 :: Dropout
transformer.transformer_blocks.4.attn.to_add_out :: Linear
transformer.transformer_blocks.4.attn.norm_added_q :: RMSNorm
transformer.transformer_blocks.4.attn.norm_added_k :: RMSNorm
transformer.transformer_blocks.4.attn2 :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.4.attn2.norm_q :: RMSNorm [CROSS]
transformer.transformer_blocks.4.attn2.norm_k :: RMSNorm [CROSS]
transformer.transformer_blocks.4.attn2.to_q :: Linear [CROSS]
transformer.transformer_blocks.4.attn2.to_k :: Linear [CROSS]
transformer.transformer_blocks.4.attn2.to_v :: Linear [CROSS]
transformer.transformer_blocks.4.attn2.to_out :: ModuleList [CROSS]
transformer.transformer_blocks.4.attn2.to_out.0 :: Linear [CROSS]
transformer.transformer_blocks.4.attn2.to_out.1 :: Dropout [CROSS]
transformer.transformer_blocks.4.norm2 :: LayerNorm
transformer.transformer_blocks.4.ff :: FeedForward
transformer.transformer_blocks.4.ff.net :: ModuleList
transformer.transformer_blocks.4.ff.net.0 :: GELU
transformer.transformer_blocks.4.ff.net.0.proj :: Linear
transformer.transformer_blocks.4.ff.net.1 :: Dropout
transformer.transformer_blocks.4.ff.net.2 :: Linear
transformer.transformer_blocks.4.norm2_context :: LayerNorm
transformer.transformer_blocks.4.ff_context :: FeedForward
transformer.transformer_blocks.4.ff_context.net :: ModuleList
transformer.transformer_blocks.4.ff_context.net.0 :: GELU
transformer.transformer_blocks.4.ff_context.net.0.proj :: Linear
transformer.transformer_blocks.4.ff_context.net.1 :: Dropout
transformer.transformer_blocks.4.ff_context.net.2 :: Linear
transformer.transformer_blocks.5 :: JointTransformerBlock
transformer.transformer_blocks.5.norm1 :: SD35AdaLayerNormZeroX
transformer.transformer_blocks.5.norm1.silu :: SiLU
transformer.transformer_blocks.5.norm1.linear :: Linear
transformer.transformer_blocks.5.norm1.norm :: LayerNorm
transformer.transformer_blocks.5.norm1_context :: AdaLayerNormZero
transformer.transformer_blocks.5.norm1_context.silu :: SiLU
transformer.transformer_blocks.5.norm1_context.linear :: Linear
transformer.transformer_blocks.5.norm1_context.norm :: LayerNorm
transformer.transformer_blocks.5.attn :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.5.attn.norm_q :: RMSNorm
transformer.transformer_blocks.5.attn.norm_k :: RMSNorm
transformer.transformer_blocks.5.attn.to_q :: Linear
transformer.transformer_blocks.5.attn.to_k :: Linear
transformer.transformer_blocks.5.attn.to_v :: Linear
transformer.transformer_blocks.5.attn.add_k_proj :: Linear
transformer.transformer_blocks.5.attn.add_v_proj :: Linear
transformer.transformer_blocks.5.attn.add_q_proj :: Linear
transformer.transformer_blocks.5.attn.to_out :: ModuleList
transformer.transformer_blocks.5.attn.to_out.0 :: Linear
transformer.transformer_blocks.5.attn.to_out.1 :: Dropout
transformer.transformer_blocks.5.attn.to_add_out :: Linear
transformer.transformer_blocks.5.attn.norm_added_q :: RMSNorm
transformer.transformer_blocks.5.attn.norm_added_k :: RMSNorm
transformer.transformer_blocks.5.attn2 :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.5.attn2.norm_q :: RMSNorm [CROSS]
transformer.transformer_blocks.5.attn2.norm_k :: RMSNorm [CROSS]
transformer.transformer_blocks.5.attn2.to_q :: Linear [CROSS]
transformer.transformer_blocks.5.attn2.to_k :: Linear [CROSS]
transformer.transformer_blocks.5.attn2.to_v :: Linear [CROSS]
transformer.transformer_blocks.5.attn2.to_out :: ModuleList [CROSS]
transformer.transformer_blocks.5.attn2.to_out.0 :: Linear [CROSS]
transformer.transformer_blocks.5.attn2.to_out.1 :: Dropout [CROSS]
transformer.transformer_blocks.5.norm2 :: LayerNorm
transformer.transformer_blocks.5.ff :: FeedForward
transformer.transformer_blocks.5.ff.net :: ModuleList
transformer.transformer_blocks.5.ff.net.0 :: GELU
transformer.transformer_blocks.5.ff.net.0.proj :: Linear
transformer.transformer_blocks.5.ff.net.1 :: Dropout
transformer.transformer_blocks.5.ff.net.2 :: Linear
transformer.transformer_blocks.5.norm2_context :: LayerNorm
transformer.transformer_blocks.5.ff_context :: FeedForward
transformer.transformer_blocks.5.ff_context.net :: ModuleList
transformer.transformer_blocks.5.ff_context.net.0 :: GELU
transformer.transformer_blocks.5.ff_context.net.0.proj :: Linear
transformer.transformer_blocks.5.ff_context.net.1 :: Dropout
transformer.transformer_blocks.5.ff_context.net.2 :: Linear
transformer.transformer_blocks.6 :: JointTransformerBlock
transformer.transformer_blocks.6.norm1 :: SD35AdaLayerNormZeroX
transformer.transformer_blocks.6.norm1.silu :: SiLU
transformer.transformer_blocks.6.norm1.linear :: Linear
transformer.transformer_blocks.6.norm1.norm :: LayerNorm
transformer.transformer_blocks.6.norm1_context :: AdaLayerNormZero
transformer.transformer_blocks.6.norm1_context.silu :: SiLU
transformer.transformer_blocks.6.norm1_context.linear :: Linear
transformer.transformer_blocks.6.norm1_context.norm :: LayerNorm
transformer.transformer_blocks.6.attn :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.6.attn.norm_q :: RMSNorm
transformer.transformer_blocks.6.attn.norm_k :: RMSNorm
transformer.transformer_blocks.6.attn.to_q :: Linear
transformer.transformer_blocks.6.attn.to_k :: Linear
transformer.transformer_blocks.6.attn.to_v :: Linear
transformer.transformer_blocks.6.attn.add_k_proj :: Linear
transformer.transformer_blocks.6.attn.add_v_proj :: Linear
transformer.transformer_blocks.6.attn.add_q_proj :: Linear
transformer.transformer_blocks.6.attn.to_out :: ModuleList
transformer.transformer_blocks.6.attn.to_out.0 :: Linear
transformer.transformer_blocks.6.attn.to_out.1 :: Dropout
transformer.transformer_blocks.6.attn.to_add_out :: Linear
transformer.transformer_blocks.6.attn.norm_added_q :: RMSNorm
transformer.transformer_blocks.6.attn.norm_added_k :: RMSNorm
transformer.transformer_blocks.6.attn2 :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.6.attn2.norm_q :: RMSNorm [CROSS]
transformer.transformer_blocks.6.attn2.norm_k :: RMSNorm [CROSS]
transformer.transformer_blocks.6.attn2.to_q :: Linear [CROSS]
transformer.transformer_blocks.6.attn2.to_k :: Linear [CROSS]
transformer.transformer_blocks.6.attn2.to_v :: Linear [CROSS]
transformer.transformer_blocks.6.attn2.to_out :: ModuleList [CROSS]
transformer.transformer_blocks.6.attn2.to_out.0 :: Linear [CROSS]
transformer.transformer_blocks.6.attn2.to_out.1 :: Dropout [CROSS]
transformer.transformer_blocks.6.norm2 :: LayerNorm
transformer.transformer_blocks.6.ff :: FeedForward
transformer.transformer_blocks.6.ff.net :: ModuleList
transformer.transformer_blocks.6.ff.net.0 :: GELU
transformer.transformer_blocks.6.ff.net.0.proj :: Linear
transformer.transformer_blocks.6.ff.net.1 :: Dropout
transformer.transformer_blocks.6.ff.net.2 :: Linear
transformer.transformer_blocks.6.norm2_context :: LayerNorm
transformer.transformer_blocks.6.ff_context :: FeedForward
transformer.transformer_blocks.6.ff_context.net :: ModuleList
transformer.transformer_blocks.6.ff_context.net.0 :: GELU
transformer.transformer_blocks.6.ff_context.net.0.proj :: Linear
transformer.transformer_blocks.6.ff_context.net.1 :: Dropout
transformer.transformer_blocks.6.ff_context.net.2 :: Linear
transformer.transformer_blocks.7 :: JointTransformerBlock
transformer.transformer_blocks.7.norm1 :: SD35AdaLayerNormZeroX
transformer.transformer_blocks.7.norm1.silu :: SiLU
transformer.transformer_blocks.7.norm1.linear :: Linear
transformer.transformer_blocks.7.norm1.norm :: LayerNorm
transformer.transformer_blocks.7.norm1_context :: AdaLayerNormZero
transformer.transformer_blocks.7.norm1_context.silu :: SiLU
transformer.transformer_blocks.7.norm1_context.linear :: Linear
transformer.transformer_blocks.7.norm1_context.norm :: LayerNorm
transformer.transformer_blocks.7.attn :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.7.attn.norm_q :: RMSNorm
transformer.transformer_blocks.7.attn.norm_k :: RMSNorm
transformer.transformer_blocks.7.attn.to_q :: Linear
transformer.transformer_blocks.7.attn.to_k :: Linear
transformer.transformer_blocks.7.attn.to_v :: Linear
transformer.transformer_blocks.7.attn.add_k_proj :: Linear
transformer.transformer_blocks.7.attn.add_v_proj :: Linear
transformer.transformer_blocks.7.attn.add_q_proj :: Linear
transformer.transformer_blocks.7.attn.to_out :: ModuleList
transformer.transformer_blocks.7.attn.to_out.0 :: Linear
transformer.transformer_blocks.7.attn.to_out.1 :: Dropout
transformer.transformer_blocks.7.attn.to_add_out :: Linear
transformer.transformer_blocks.7.attn.norm_added_q :: RMSNorm
transformer.transformer_blocks.7.attn.norm_added_k :: RMSNorm
transformer.transformer_blocks.7.attn2 :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.7.attn2.norm_q :: RMSNorm [CROSS]
transformer.transformer_blocks.7.attn2.norm_k :: RMSNorm [CROSS]
transformer.transformer_blocks.7.attn2.to_q :: Linear [CROSS]
transformer.transformer_blocks.7.attn2.to_k :: Linear [CROSS]
transformer.transformer_blocks.7.attn2.to_v :: Linear [CROSS]
transformer.transformer_blocks.7.attn2.to_out :: ModuleList [CROSS]
transformer.transformer_blocks.7.attn2.to_out.0 :: Linear [CROSS]
transformer.transformer_blocks.7.attn2.to_out.1 :: Dropout [CROSS]
transformer.transformer_blocks.7.norm2 :: LayerNorm
transformer.transformer_blocks.7.ff :: FeedForward
transformer.transformer_blocks.7.ff.net :: ModuleList
transformer.transformer_blocks.7.ff.net.0 :: GELU
transformer.transformer_blocks.7.ff.net.0.proj :: Linear
transformer.transformer_blocks.7.ff.net.1 :: Dropout
transformer.transformer_blocks.7.ff.net.2 :: Linear
transformer.transformer_blocks.7.norm2_context :: LayerNorm
transformer.transformer_blocks.7.ff_context :: FeedForward
transformer.transformer_blocks.7.ff_context.net :: ModuleList
transformer.transformer_blocks.7.ff_context.net.0 :: GELU
transformer.transformer_blocks.7.ff_context.net.0.proj :: Linear
transformer.transformer_blocks.7.ff_context.net.1 :: Dropout
transformer.transformer_blocks.7.ff_context.net.2 :: Linear
transformer.transformer_blocks.8 :: JointTransformerBlock
transformer.transformer_blocks.8.norm1 :: SD35AdaLayerNormZeroX
transformer.transformer_blocks.8.norm1.silu :: SiLU
transformer.transformer_blocks.8.norm1.linear :: Linear
transformer.transformer_blocks.8.norm1.norm :: LayerNorm
transformer.transformer_blocks.8.norm1_context :: AdaLayerNormZero
transformer.transformer_blocks.8.norm1_context.silu :: SiLU
transformer.transformer_blocks.8.norm1_context.linear :: Linear
transformer.transformer_blocks.8.norm1_context.norm :: LayerNorm
transformer.transformer_blocks.8.attn :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.8.attn.norm_q :: RMSNorm
transformer.transformer_blocks.8.attn.norm_k :: RMSNorm
transformer.transformer_blocks.8.attn.to_q :: Linear
transformer.transformer_blocks.8.attn.to_k :: Linear
transformer.transformer_blocks.8.attn.to_v :: Linear
transformer.transformer_blocks.8.attn.add_k_proj :: Linear
transformer.transformer_blocks.8.attn.add_v_proj :: Linear
transformer.transformer_blocks.8.attn.add_q_proj :: Linear
transformer.transformer_blocks.8.attn.to_out :: ModuleList
transformer.transformer_blocks.8.attn.to_out.0 :: Linear
transformer.transformer_blocks.8.attn.to_out.1 :: Dropout
transformer.transformer_blocks.8.attn.to_add_out :: Linear
transformer.transformer_blocks.8.attn.norm_added_q :: RMSNorm
transformer.transformer_blocks.8.attn.norm_added_k :: RMSNorm
transformer.transformer_blocks.8.attn2 :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.8.attn2.norm_q :: RMSNorm [CROSS]
transformer.transformer_blocks.8.attn2.norm_k :: RMSNorm [CROSS]
transformer.transformer_blocks.8.attn2.to_q :: Linear [CROSS]
transformer.transformer_blocks.8.attn2.to_k :: Linear [CROSS]
transformer.transformer_blocks.8.attn2.to_v :: Linear [CROSS]
transformer.transformer_blocks.8.attn2.to_out :: ModuleList [CROSS]
transformer.transformer_blocks.8.attn2.to_out.0 :: Linear [CROSS]
transformer.transformer_blocks.8.attn2.to_out.1 :: Dropout [CROSS]
transformer.transformer_blocks.8.norm2 :: LayerNorm
transformer.transformer_blocks.8.ff :: FeedForward
transformer.transformer_blocks.8.ff.net :: ModuleList
transformer.transformer_blocks.8.ff.net.0 :: GELU
transformer.transformer_blocks.8.ff.net.0.proj :: Linear
transformer.transformer_blocks.8.ff.net.1 :: Dropout
transformer.transformer_blocks.8.ff.net.2 :: Linear
transformer.transformer_blocks.8.norm2_context :: LayerNorm
transformer.transformer_blocks.8.ff_context :: FeedForward
transformer.transformer_blocks.8.ff_context.net :: ModuleList
transformer.transformer_blocks.8.ff_context.net.0 :: GELU
transformer.transformer_blocks.8.ff_context.net.0.proj :: Linear
transformer.transformer_blocks.8.ff_context.net.1 :: Dropout
transformer.transformer_blocks.8.ff_context.net.2 :: Linear
transformer.transformer_blocks.9 :: JointTransformerBlock
transformer.transformer_blocks.9.norm1 :: SD35AdaLayerNormZeroX
transformer.transformer_blocks.9.norm1.silu :: SiLU
transformer.transformer_blocks.9.norm1.linear :: Linear
transformer.transformer_blocks.9.norm1.norm :: LayerNorm
transformer.transformer_blocks.9.norm1_context :: AdaLayerNormZero
transformer.transformer_blocks.9.norm1_context.silu :: SiLU
transformer.transformer_blocks.9.norm1_context.linear :: Linear
transformer.transformer_blocks.9.norm1_context.norm :: LayerNorm
transformer.transformer_blocks.9.attn :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.9.attn.norm_q :: RMSNorm
transformer.transformer_blocks.9.attn.norm_k :: RMSNorm
transformer.transformer_blocks.9.attn.to_q :: Linear
transformer.transformer_blocks.9.attn.to_k :: Linear
transformer.transformer_blocks.9.attn.to_v :: Linear
transformer.transformer_blocks.9.attn.add_k_proj :: Linear
transformer.transformer_blocks.9.attn.add_v_proj :: Linear
transformer.transformer_blocks.9.attn.add_q_proj :: Linear
transformer.transformer_blocks.9.attn.to_out :: ModuleList
transformer.transformer_blocks.9.attn.to_out.0 :: Linear
transformer.transformer_blocks.9.attn.to_out.1 :: Dropout
transformer.transformer_blocks.9.attn.to_add_out :: Linear
transformer.transformer_blocks.9.attn.norm_added_q :: RMSNorm
transformer.transformer_blocks.9.attn.norm_added_k :: RMSNorm
transformer.transformer_blocks.9.attn2 :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.9.attn2.norm_q :: RMSNorm [CROSS]
transformer.transformer_blocks.9.attn2.norm_k :: RMSNorm [CROSS]
transformer.transformer_blocks.9.attn2.to_q :: Linear [CROSS]
transformer.transformer_blocks.9.attn2.to_k :: Linear [CROSS]
transformer.transformer_blocks.9.attn2.to_v :: Linear [CROSS]
transformer.transformer_blocks.9.attn2.to_out :: ModuleList [CROSS]
transformer.transformer_blocks.9.attn2.to_out.0 :: Linear [CROSS]
transformer.transformer_blocks.9.attn2.to_out.1 :: Dropout [CROSS]
transformer.transformer_blocks.9.norm2 :: LayerNorm
transformer.transformer_blocks.9.ff :: FeedForward
transformer.transformer_blocks.9.ff.net :: ModuleList
transformer.transformer_blocks.9.ff.net.0 :: GELU
transformer.transformer_blocks.9.ff.net.0.proj :: Linear
transformer.transformer_blocks.9.ff.net.1 :: Dropout
transformer.transformer_blocks.9.ff.net.2 :: Linear
transformer.transformer_blocks.9.norm2_context :: LayerNorm
transformer.transformer_blocks.9.ff_context :: FeedForward
transformer.transformer_blocks.9.ff_context.net :: ModuleList
transformer.transformer_blocks.9.ff_context.net.0 :: GELU
transformer.transformer_blocks.9.ff_context.net.0.proj :: Linear
transformer.transformer_blocks.9.ff_context.net.1 :: Dropout
transformer.transformer_blocks.9.ff_context.net.2 :: Linear
transformer.transformer_blocks.10 :: JointTransformerBlock
transformer.transformer_blocks.10.norm1 :: SD35AdaLayerNormZeroX
transformer.transformer_blocks.10.norm1.silu :: SiLU
transformer.transformer_blocks.10.norm1.linear :: Linear
transformer.transformer_blocks.10.norm1.norm :: LayerNorm
transformer.transformer_blocks.10.norm1_context :: AdaLayerNormZero
transformer.transformer_blocks.10.norm1_context.silu :: SiLU
transformer.transformer_blocks.10.norm1_context.linear :: Linear
transformer.transformer_blocks.10.norm1_context.norm :: LayerNorm
transformer.transformer_blocks.10.attn :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.10.attn.norm_q :: RMSNorm
transformer.transformer_blocks.10.attn.norm_k :: RMSNorm
transformer.transformer_blocks.10.attn.to_q :: Linear
transformer.transformer_blocks.10.attn.to_k :: Linear
transformer.transformer_blocks.10.attn.to_v :: Linear
transformer.transformer_blocks.10.attn.add_k_proj :: Linear
transformer.transformer_blocks.10.attn.add_v_proj :: Linear
transformer.transformer_blocks.10.attn.add_q_proj :: Linear
transformer.transformer_blocks.10.attn.to_out :: ModuleList
transformer.transformer_blocks.10.attn.to_out.0 :: Linear
transformer.transformer_blocks.10.attn.to_out.1 :: Dropout
transformer.transformer_blocks.10.attn.to_add_out :: Linear
transformer.transformer_blocks.10.attn.norm_added_q :: RMSNorm
transformer.transformer_blocks.10.attn.norm_added_k :: RMSNorm
transformer.transformer_blocks.10.attn2 :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.10.attn2.norm_q :: RMSNorm [CROSS]
transformer.transformer_blocks.10.attn2.norm_k :: RMSNorm [CROSS]
transformer.transformer_blocks.10.attn2.to_q :: Linear [CROSS]
transformer.transformer_blocks.10.attn2.to_k :: Linear [CROSS]
transformer.transformer_blocks.10.attn2.to_v :: Linear [CROSS]
transformer.transformer_blocks.10.attn2.to_out :: ModuleList [CROSS]
transformer.transformer_blocks.10.attn2.to_out.0 :: Linear [CROSS]
transformer.transformer_blocks.10.attn2.to_out.1 :: Dropout [CROSS]
transformer.transformer_blocks.10.norm2 :: LayerNorm
transformer.transformer_blocks.10.ff :: FeedForward
transformer.transformer_blocks.10.ff.net :: ModuleList
transformer.transformer_blocks.10.ff.net.0 :: GELU
transformer.transformer_blocks.10.ff.net.0.proj :: Linear
transformer.transformer_blocks.10.ff.net.1 :: Dropout
transformer.transformer_blocks.10.ff.net.2 :: Linear
transformer.transformer_blocks.10.norm2_context :: LayerNorm
transformer.transformer_blocks.10.ff_context :: FeedForward
transformer.transformer_blocks.10.ff_context.net :: ModuleList
transformer.transformer_blocks.10.ff_context.net.0 :: GELU
transformer.transformer_blocks.10.ff_context.net.0.proj :: Linear
transformer.transformer_blocks.10.ff_context.net.1 :: Dropout
transformer.transformer_blocks.10.ff_context.net.2 :: Linear
transformer.transformer_blocks.11 :: JointTransformerBlock
transformer.transformer_blocks.11.norm1 :: SD35AdaLayerNormZeroX
transformer.transformer_blocks.11.norm1.silu :: SiLU
transformer.transformer_blocks.11.norm1.linear :: Linear
transformer.transformer_blocks.11.norm1.norm :: LayerNorm
transformer.transformer_blocks.11.norm1_context :: AdaLayerNormZero
transformer.transformer_blocks.11.norm1_context.silu :: SiLU
transformer.transformer_blocks.11.norm1_context.linear :: Linear
transformer.transformer_blocks.11.norm1_context.norm :: LayerNorm
transformer.transformer_blocks.11.attn :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.11.attn.norm_q :: RMSNorm
transformer.transformer_blocks.11.attn.norm_k :: RMSNorm
transformer.transformer_blocks.11.attn.to_q :: Linear
transformer.transformer_blocks.11.attn.to_k :: Linear
transformer.transformer_blocks.11.attn.to_v :: Linear
transformer.transformer_blocks.11.attn.add_k_proj :: Linear
transformer.transformer_blocks.11.attn.add_v_proj :: Linear
transformer.transformer_blocks.11.attn.add_q_proj :: Linear
transformer.transformer_blocks.11.attn.to_out :: ModuleList
transformer.transformer_blocks.11.attn.to_out.0 :: Linear
transformer.transformer_blocks.11.attn.to_out.1 :: Dropout
transformer.transformer_blocks.11.attn.to_add_out :: Linear
transformer.transformer_blocks.11.attn.norm_added_q :: RMSNorm
transformer.transformer_blocks.11.attn.norm_added_k :: RMSNorm
transformer.transformer_blocks.11.attn2 :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.11.attn2.norm_q :: RMSNorm [CROSS]
transformer.transformer_blocks.11.attn2.norm_k :: RMSNorm [CROSS]
transformer.transformer_blocks.11.attn2.to_q :: Linear [CROSS]
transformer.transformer_blocks.11.attn2.to_k :: Linear [CROSS]
transformer.transformer_blocks.11.attn2.to_v :: Linear [CROSS]
transformer.transformer_blocks.11.attn2.to_out :: ModuleList [CROSS]
transformer.transformer_blocks.11.attn2.to_out.0 :: Linear [CROSS]
transformer.transformer_blocks.11.attn2.to_out.1 :: Dropout [CROSS]
transformer.transformer_blocks.11.norm2 :: LayerNorm
transformer.transformer_blocks.11.ff :: FeedForward
transformer.transformer_blocks.11.ff.net :: ModuleList
transformer.transformer_blocks.11.ff.net.0 :: GELU
transformer.transformer_blocks.11.ff.net.0.proj :: Linear
transformer.transformer_blocks.11.ff.net.1 :: Dropout
transformer.transformer_blocks.11.ff.net.2 :: Linear
transformer.transformer_blocks.11.norm2_context :: LayerNorm
transformer.transformer_blocks.11.ff_context :: FeedForward
transformer.transformer_blocks.11.ff_context.net :: ModuleList
transformer.transformer_blocks.11.ff_context.net.0 :: GELU
transformer.transformer_blocks.11.ff_context.net.0.proj :: Linear
transformer.transformer_blocks.11.ff_context.net.1 :: Dropout
transformer.transformer_blocks.11.ff_context.net.2 :: Linear
transformer.transformer_blocks.12 :: JointTransformerBlock
transformer.transformer_blocks.12.norm1 :: SD35AdaLayerNormZeroX
transformer.transformer_blocks.12.norm1.silu :: SiLU
transformer.transformer_blocks.12.norm1.linear :: Linear
transformer.transformer_blocks.12.norm1.norm :: LayerNorm
transformer.transformer_blocks.12.norm1_context :: AdaLayerNormZero
transformer.transformer_blocks.12.norm1_context.silu :: SiLU
transformer.transformer_blocks.12.norm1_context.linear :: Linear
transformer.transformer_blocks.12.norm1_context.norm :: LayerNorm
transformer.transformer_blocks.12.attn :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.12.attn.norm_q :: RMSNorm
transformer.transformer_blocks.12.attn.norm_k :: RMSNorm
transformer.transformer_blocks.12.attn.to_q :: Linear
transformer.transformer_blocks.12.attn.to_k :: Linear
transformer.transformer_blocks.12.attn.to_v :: Linear
transformer.transformer_blocks.12.attn.add_k_proj :: Linear
transformer.transformer_blocks.12.attn.add_v_proj :: Linear
transformer.transformer_blocks.12.attn.add_q_proj :: Linear
transformer.transformer_blocks.12.attn.to_out :: ModuleList
transformer.transformer_blocks.12.attn.to_out.0 :: Linear
transformer.transformer_blocks.12.attn.to_out.1 :: Dropout
transformer.transformer_blocks.12.attn.to_add_out :: Linear
transformer.transformer_blocks.12.attn.norm_added_q :: RMSNorm
transformer.transformer_blocks.12.attn.norm_added_k :: RMSNorm
transformer.transformer_blocks.12.attn2 :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.12.attn2.norm_q :: RMSNorm [CROSS]
transformer.transformer_blocks.12.attn2.norm_k :: RMSNorm [CROSS]
transformer.transformer_blocks.12.attn2.to_q :: Linear [CROSS]
transformer.transformer_blocks.12.attn2.to_k :: Linear [CROSS]
transformer.transformer_blocks.12.attn2.to_v :: Linear [CROSS]
transformer.transformer_blocks.12.attn2.to_out :: ModuleList [CROSS]
transformer.transformer_blocks.12.attn2.to_out.0 :: Linear [CROSS]
transformer.transformer_blocks.12.attn2.to_out.1 :: Dropout [CROSS]
transformer.transformer_blocks.12.norm2 :: LayerNorm
transformer.transformer_blocks.12.ff :: FeedForward
transformer.transformer_blocks.12.ff.net :: ModuleList
transformer.transformer_blocks.12.ff.net.0 :: GELU
transformer.transformer_blocks.12.ff.net.0.proj :: Linear
transformer.transformer_blocks.12.ff.net.1 :: Dropout
transformer.transformer_blocks.12.ff.net.2 :: Linear
transformer.transformer_blocks.12.norm2_context :: LayerNorm
transformer.transformer_blocks.12.ff_context :: FeedForward
transformer.transformer_blocks.12.ff_context.net :: ModuleList
transformer.transformer_blocks.12.ff_context.net.0 :: GELU
transformer.transformer_blocks.12.ff_context.net.0.proj :: Linear
transformer.transformer_blocks.12.ff_context.net.1 :: Dropout
transformer.transformer_blocks.12.ff_context.net.2 :: Linear
transformer.transformer_blocks.13 :: JointTransformerBlock
transformer.transformer_blocks.13.norm1 :: AdaLayerNormZero
transformer.transformer_blocks.13.norm1.silu :: SiLU
transformer.transformer_blocks.13.norm1.linear :: Linear
transformer.transformer_blocks.13.norm1.norm :: LayerNorm
transformer.transformer_blocks.13.norm1_context :: AdaLayerNormZero
transformer.transformer_blocks.13.norm1_context.silu :: SiLU
transformer.transformer_blocks.13.norm1_context.linear :: Linear
transformer.transformer_blocks.13.norm1_context.norm :: LayerNorm
transformer.transformer_blocks.13.attn :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.13.attn.norm_q :: RMSNorm
transformer.transformer_blocks.13.attn.norm_k :: RMSNorm
transformer.transformer_blocks.13.attn.to_q :: Linear
transformer.transformer_blocks.13.attn.to_k :: Linear
transformer.transformer_blocks.13.attn.to_v :: Linear
transformer.transformer_blocks.13.attn.add_k_proj :: Linear
transformer.transformer_blocks.13.attn.add_v_proj :: Linear
transformer.transformer_blocks.13.attn.add_q_proj :: Linear
transformer.transformer_blocks.13.attn.to_out :: ModuleList
transformer.transformer_blocks.13.attn.to_out.0 :: Linear
transformer.transformer_blocks.13.attn.to_out.1 :: Dropout
transformer.transformer_blocks.13.attn.to_add_out :: Linear
transformer.transformer_blocks.13.attn.norm_added_q :: RMSNorm
transformer.transformer_blocks.13.attn.norm_added_k :: RMSNorm
transformer.transformer_blocks.13.norm2 :: LayerNorm
transformer.transformer_blocks.13.ff :: FeedForward
transformer.transformer_blocks.13.ff.net :: ModuleList
transformer.transformer_blocks.13.ff.net.0 :: GELU
transformer.transformer_blocks.13.ff.net.0.proj :: Linear
transformer.transformer_blocks.13.ff.net.1 :: Dropout
transformer.transformer_blocks.13.ff.net.2 :: Linear
transformer.transformer_blocks.13.norm2_context :: LayerNorm
transformer.transformer_blocks.13.ff_context :: FeedForward
transformer.transformer_blocks.13.ff_context.net :: ModuleList
transformer.transformer_blocks.13.ff_context.net.0 :: GELU
transformer.transformer_blocks.13.ff_context.net.0.proj :: Linear
transformer.transformer_blocks.13.ff_context.net.1 :: Dropout
transformer.transformer_blocks.13.ff_context.net.2 :: Linear
transformer.transformer_blocks.14 :: JointTransformerBlock
transformer.transformer_blocks.14.norm1 :: AdaLayerNormZero
transformer.transformer_blocks.14.norm1.silu :: SiLU
transformer.transformer_blocks.14.norm1.linear :: Linear
transformer.transformer_blocks.14.norm1.norm :: LayerNorm
transformer.transformer_blocks.14.norm1_context :: AdaLayerNormZero
transformer.transformer_blocks.14.norm1_context.silu :: SiLU
transformer.transformer_blocks.14.norm1_context.linear :: Linear
transformer.transformer_blocks.14.norm1_context.norm :: LayerNorm
transformer.transformer_blocks.14.attn :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.14.attn.norm_q :: RMSNorm
transformer.transformer_blocks.14.attn.norm_k :: RMSNorm
transformer.transformer_blocks.14.attn.to_q :: Linear
transformer.transformer_blocks.14.attn.to_k :: Linear
transformer.transformer_blocks.14.attn.to_v :: Linear
transformer.transformer_blocks.14.attn.add_k_proj :: Linear
transformer.transformer_blocks.14.attn.add_v_proj :: Linear
transformer.transformer_blocks.14.attn.add_q_proj :: Linear
transformer.transformer_blocks.14.attn.to_out :: ModuleList
transformer.transformer_blocks.14.attn.to_out.0 :: Linear
transformer.transformer_blocks.14.attn.to_out.1 :: Dropout
transformer.transformer_blocks.14.attn.to_add_out :: Linear
transformer.transformer_blocks.14.attn.norm_added_q :: RMSNorm
transformer.transformer_blocks.14.attn.norm_added_k :: RMSNorm
transformer.transformer_blocks.14.norm2 :: LayerNorm
transformer.transformer_blocks.14.ff :: FeedForward
transformer.transformer_blocks.14.ff.net :: ModuleList
transformer.transformer_blocks.14.ff.net.0 :: GELU
transformer.transformer_blocks.14.ff.net.0.proj :: Linear
transformer.transformer_blocks.14.ff.net.1 :: Dropout
transformer.transformer_blocks.14.ff.net.2 :: Linear
transformer.transformer_blocks.14.norm2_context :: LayerNorm
transformer.transformer_blocks.14.ff_context :: FeedForward
transformer.transformer_blocks.14.ff_context.net :: ModuleList
transformer.transformer_blocks.14.ff_context.net.0 :: GELU
transformer.transformer_blocks.14.ff_context.net.0.proj :: Linear
transformer.transformer_blocks.14.ff_context.net.1 :: Dropout
transformer.transformer_blocks.14.ff_context.net.2 :: Linear
transformer.transformer_blocks.15 :: JointTransformerBlock
transformer.transformer_blocks.15.norm1 :: AdaLayerNormZero
transformer.transformer_blocks.15.norm1.silu :: SiLU
transformer.transformer_blocks.15.norm1.linear :: Linear
transformer.transformer_blocks.15.norm1.norm :: LayerNorm
transformer.transformer_blocks.15.norm1_context :: AdaLayerNormZero
transformer.transformer_blocks.15.norm1_context.silu :: SiLU
transformer.transformer_blocks.15.norm1_context.linear :: Linear
transformer.transformer_blocks.15.norm1_context.norm :: LayerNorm
transformer.transformer_blocks.15.attn :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.15.attn.norm_q :: RMSNorm
transformer.transformer_blocks.15.attn.norm_k :: RMSNorm
transformer.transformer_blocks.15.attn.to_q :: Linear
transformer.transformer_blocks.15.attn.to_k :: Linear
transformer.transformer_blocks.15.attn.to_v :: Linear
transformer.transformer_blocks.15.attn.add_k_proj :: Linear
transformer.transformer_blocks.15.attn.add_v_proj :: Linear
transformer.transformer_blocks.15.attn.add_q_proj :: Linear
transformer.transformer_blocks.15.attn.to_out :: ModuleList
transformer.transformer_blocks.15.attn.to_out.0 :: Linear
transformer.transformer_blocks.15.attn.to_out.1 :: Dropout
transformer.transformer_blocks.15.attn.to_add_out :: Linear
transformer.transformer_blocks.15.attn.norm_added_q :: RMSNorm
transformer.transformer_blocks.15.attn.norm_added_k :: RMSNorm
transformer.transformer_blocks.15.norm2 :: LayerNorm
transformer.transformer_blocks.15.ff :: FeedForward
transformer.transformer_blocks.15.ff.net :: ModuleList
transformer.transformer_blocks.15.ff.net.0 :: GELU
transformer.transformer_blocks.15.ff.net.0.proj :: Linear
transformer.transformer_blocks.15.ff.net.1 :: Dropout
transformer.transformer_blocks.15.ff.net.2 :: Linear
transformer.transformer_blocks.15.norm2_context :: LayerNorm
transformer.transformer_blocks.15.ff_context :: FeedForward
transformer.transformer_blocks.15.ff_context.net :: ModuleList
transformer.transformer_blocks.15.ff_context.net.0 :: GELU
transformer.transformer_blocks.15.ff_context.net.0.proj :: Linear
transformer.transformer_blocks.15.ff_context.net.1 :: Dropout
transformer.transformer_blocks.15.ff_context.net.2 :: Linear
transformer.transformer_blocks.16 :: JointTransformerBlock
transformer.transformer_blocks.16.norm1 :: AdaLayerNormZero
transformer.transformer_blocks.16.norm1.silu :: SiLU
transformer.transformer_blocks.16.norm1.linear :: Linear
transformer.transformer_blocks.16.norm1.norm :: LayerNorm
transformer.transformer_blocks.16.norm1_context :: AdaLayerNormZero
transformer.transformer_blocks.16.norm1_context.silu :: SiLU
transformer.transformer_blocks.16.norm1_context.linear :: Linear
transformer.transformer_blocks.16.norm1_context.norm :: LayerNorm
transformer.transformer_blocks.16.attn :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.16.attn.norm_q :: RMSNorm
transformer.transformer_blocks.16.attn.norm_k :: RMSNorm
transformer.transformer_blocks.16.attn.to_q :: Linear
transformer.transformer_blocks.16.attn.to_k :: Linear
transformer.transformer_blocks.16.attn.to_v :: Linear
transformer.transformer_blocks.16.attn.add_k_proj :: Linear
transformer.transformer_blocks.16.attn.add_v_proj :: Linear
transformer.transformer_blocks.16.attn.add_q_proj :: Linear
transformer.transformer_blocks.16.attn.to_out :: ModuleList
transformer.transformer_blocks.16.attn.to_out.0 :: Linear
transformer.transformer_blocks.16.attn.to_out.1 :: Dropout
transformer.transformer_blocks.16.attn.to_add_out :: Linear
transformer.transformer_blocks.16.attn.norm_added_q :: RMSNorm
transformer.transformer_blocks.16.attn.norm_added_k :: RMSNorm
transformer.transformer_blocks.16.norm2 :: LayerNorm
transformer.transformer_blocks.16.ff :: FeedForward
transformer.transformer_blocks.16.ff.net :: ModuleList
transformer.transformer_blocks.16.ff.net.0 :: GELU
transformer.transformer_blocks.16.ff.net.0.proj :: Linear
transformer.transformer_blocks.16.ff.net.1 :: Dropout
transformer.transformer_blocks.16.ff.net.2 :: Linear
transformer.transformer_blocks.16.norm2_context :: LayerNorm
transformer.transformer_blocks.16.ff_context :: FeedForward
transformer.transformer_blocks.16.ff_context.net :: ModuleList
transformer.transformer_blocks.16.ff_context.net.0 :: GELU
transformer.transformer_blocks.16.ff_context.net.0.proj :: Linear
transformer.transformer_blocks.16.ff_context.net.1 :: Dropout
transformer.transformer_blocks.16.ff_context.net.2 :: Linear
transformer.transformer_blocks.17 :: JointTransformerBlock
transformer.transformer_blocks.17.norm1 :: AdaLayerNormZero
transformer.transformer_blocks.17.norm1.silu :: SiLU
transformer.transformer_blocks.17.norm1.linear :: Linear
transformer.transformer_blocks.17.norm1.norm :: LayerNorm
transformer.transformer_blocks.17.norm1_context :: AdaLayerNormZero
transformer.transformer_blocks.17.norm1_context.silu :: SiLU
transformer.transformer_blocks.17.norm1_context.linear :: Linear
transformer.transformer_blocks.17.norm1_context.norm :: LayerNorm
transformer.transformer_blocks.17.attn :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.17.attn.norm_q :: RMSNorm
transformer.transformer_blocks.17.attn.norm_k :: RMSNorm
transformer.transformer_blocks.17.attn.to_q :: Linear
transformer.transformer_blocks.17.attn.to_k :: Linear
transformer.transformer_blocks.17.attn.to_v :: Linear
transformer.transformer_blocks.17.attn.add_k_proj :: Linear
transformer.transformer_blocks.17.attn.add_v_proj :: Linear
transformer.transformer_blocks.17.attn.add_q_proj :: Linear
transformer.transformer_blocks.17.attn.to_out :: ModuleList
transformer.transformer_blocks.17.attn.to_out.0 :: Linear
transformer.transformer_blocks.17.attn.to_out.1 :: Dropout
transformer.transformer_blocks.17.attn.to_add_out :: Linear
transformer.transformer_blocks.17.attn.norm_added_q :: RMSNorm
transformer.transformer_blocks.17.attn.norm_added_k :: RMSNorm
transformer.transformer_blocks.17.norm2 :: LayerNorm
transformer.transformer_blocks.17.ff :: FeedForward
transformer.transformer_blocks.17.ff.net :: ModuleList
transformer.transformer_blocks.17.ff.net.0 :: GELU
transformer.transformer_blocks.17.ff.net.0.proj :: Linear
transformer.transformer_blocks.17.ff.net.1 :: Dropout
transformer.transformer_blocks.17.ff.net.2 :: Linear
transformer.transformer_blocks.17.norm2_context :: LayerNorm
transformer.transformer_blocks.17.ff_context :: FeedForward
transformer.transformer_blocks.17.ff_context.net :: ModuleList
transformer.transformer_blocks.17.ff_context.net.0 :: GELU
transformer.transformer_blocks.17.ff_context.net.0.proj :: Linear
transformer.transformer_blocks.17.ff_context.net.1 :: Dropout
transformer.transformer_blocks.17.ff_context.net.2 :: Linear
transformer.transformer_blocks.18 :: JointTransformerBlock
transformer.transformer_blocks.18.norm1 :: AdaLayerNormZero
transformer.transformer_blocks.18.norm1.silu :: SiLU
transformer.transformer_blocks.18.norm1.linear :: Linear
transformer.transformer_blocks.18.norm1.norm :: LayerNorm
transformer.transformer_blocks.18.norm1_context :: AdaLayerNormZero
transformer.transformer_blocks.18.norm1_context.silu :: SiLU
transformer.transformer_blocks.18.norm1_context.linear :: Linear
transformer.transformer_blocks.18.norm1_context.norm :: LayerNorm
transformer.transformer_blocks.18.attn :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.18.attn.norm_q :: RMSNorm
transformer.transformer_blocks.18.attn.norm_k :: RMSNorm
transformer.transformer_blocks.18.attn.to_q :: Linear
transformer.transformer_blocks.18.attn.to_k :: Linear
transformer.transformer_blocks.18.attn.to_v :: Linear
transformer.transformer_blocks.18.attn.add_k_proj :: Linear
transformer.transformer_blocks.18.attn.add_v_proj :: Linear
transformer.transformer_blocks.18.attn.add_q_proj :: Linear
transformer.transformer_blocks.18.attn.to_out :: ModuleList
transformer.transformer_blocks.18.attn.to_out.0 :: Linear
transformer.transformer_blocks.18.attn.to_out.1 :: Dropout
transformer.transformer_blocks.18.attn.to_add_out :: Linear
transformer.transformer_blocks.18.attn.norm_added_q :: RMSNorm
transformer.transformer_blocks.18.attn.norm_added_k :: RMSNorm
transformer.transformer_blocks.18.norm2 :: LayerNorm
transformer.transformer_blocks.18.ff :: FeedForward
transformer.transformer_blocks.18.ff.net :: ModuleList
transformer.transformer_blocks.18.ff.net.0 :: GELU
transformer.transformer_blocks.18.ff.net.0.proj :: Linear
transformer.transformer_blocks.18.ff.net.1 :: Dropout
transformer.transformer_blocks.18.ff.net.2 :: Linear
transformer.transformer_blocks.18.norm2_context :: LayerNorm
transformer.transformer_blocks.18.ff_context :: FeedForward
transformer.transformer_blocks.18.ff_context.net :: ModuleList
transformer.transformer_blocks.18.ff_context.net.0 :: GELU
transformer.transformer_blocks.18.ff_context.net.0.proj :: Linear
transformer.transformer_blocks.18.ff_context.net.1 :: Dropout
transformer.transformer_blocks.18.ff_context.net.2 :: Linear
transformer.transformer_blocks.19 :: JointTransformerBlock
transformer.transformer_blocks.19.norm1 :: AdaLayerNormZero
transformer.transformer_blocks.19.norm1.silu :: SiLU
transformer.transformer_blocks.19.norm1.linear :: Linear
transformer.transformer_blocks.19.norm1.norm :: LayerNorm
transformer.transformer_blocks.19.norm1_context :: AdaLayerNormZero
transformer.transformer_blocks.19.norm1_context.silu :: SiLU
transformer.transformer_blocks.19.norm1_context.linear :: Linear
transformer.transformer_blocks.19.norm1_context.norm :: LayerNorm
transformer.transformer_blocks.19.attn :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.19.attn.norm_q :: RMSNorm
transformer.transformer_blocks.19.attn.norm_k :: RMSNorm
transformer.transformer_blocks.19.attn.to_q :: Linear
transformer.transformer_blocks.19.attn.to_k :: Linear
transformer.transformer_blocks.19.attn.to_v :: Linear
transformer.transformer_blocks.19.attn.add_k_proj :: Linear
transformer.transformer_blocks.19.attn.add_v_proj :: Linear
transformer.transformer_blocks.19.attn.add_q_proj :: Linear
transformer.transformer_blocks.19.attn.to_out :: ModuleList
transformer.transformer_blocks.19.attn.to_out.0 :: Linear
transformer.transformer_blocks.19.attn.to_out.1 :: Dropout
transformer.transformer_blocks.19.attn.to_add_out :: Linear
transformer.transformer_blocks.19.attn.norm_added_q :: RMSNorm
transformer.transformer_blocks.19.attn.norm_added_k :: RMSNorm
transformer.transformer_blocks.19.norm2 :: LayerNorm
transformer.transformer_blocks.19.ff :: FeedForward
transformer.transformer_blocks.19.ff.net :: ModuleList
transformer.transformer_blocks.19.ff.net.0 :: GELU
transformer.transformer_blocks.19.ff.net.0.proj :: Linear
transformer.transformer_blocks.19.ff.net.1 :: Dropout
transformer.transformer_blocks.19.ff.net.2 :: Linear
transformer.transformer_blocks.19.norm2_context :: LayerNorm
transformer.transformer_blocks.19.ff_context :: FeedForward
transformer.transformer_blocks.19.ff_context.net :: ModuleList
transformer.transformer_blocks.19.ff_context.net.0 :: GELU
transformer.transformer_blocks.19.ff_context.net.0.proj :: Linear
transformer.transformer_blocks.19.ff_context.net.1 :: Dropout
transformer.transformer_blocks.19.ff_context.net.2 :: Linear
transformer.transformer_blocks.20 :: JointTransformerBlock
transformer.transformer_blocks.20.norm1 :: AdaLayerNormZero
transformer.transformer_blocks.20.norm1.silu :: SiLU
transformer.transformer_blocks.20.norm1.linear :: Linear
transformer.transformer_blocks.20.norm1.norm :: LayerNorm
transformer.transformer_blocks.20.norm1_context :: AdaLayerNormZero
transformer.transformer_blocks.20.norm1_context.silu :: SiLU
transformer.transformer_blocks.20.norm1_context.linear :: Linear
transformer.transformer_blocks.20.norm1_context.norm :: LayerNorm
transformer.transformer_blocks.20.attn :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.20.attn.norm_q :: RMSNorm
transformer.transformer_blocks.20.attn.norm_k :: RMSNorm
transformer.transformer_blocks.20.attn.to_q :: Linear
transformer.transformer_blocks.20.attn.to_k :: Linear
transformer.transformer_blocks.20.attn.to_v :: Linear
transformer.transformer_blocks.20.attn.add_k_proj :: Linear
transformer.transformer_blocks.20.attn.add_v_proj :: Linear
transformer.transformer_blocks.20.attn.add_q_proj :: Linear
transformer.transformer_blocks.20.attn.to_out :: ModuleList
transformer.transformer_blocks.20.attn.to_out.0 :: Linear
transformer.transformer_blocks.20.attn.to_out.1 :: Dropout
transformer.transformer_blocks.20.attn.to_add_out :: Linear
transformer.transformer_blocks.20.attn.norm_added_q :: RMSNorm
transformer.transformer_blocks.20.attn.norm_added_k :: RMSNorm
transformer.transformer_blocks.20.norm2 :: LayerNorm
transformer.transformer_blocks.20.ff :: FeedForward
transformer.transformer_blocks.20.ff.net :: ModuleList
transformer.transformer_blocks.20.ff.net.0 :: GELU
transformer.transformer_blocks.20.ff.net.0.proj :: Linear
transformer.transformer_blocks.20.ff.net.1 :: Dropout
transformer.transformer_blocks.20.ff.net.2 :: Linear
transformer.transformer_blocks.20.norm2_context :: LayerNorm
transformer.transformer_blocks.20.ff_context :: FeedForward
transformer.transformer_blocks.20.ff_context.net :: ModuleList
transformer.transformer_blocks.20.ff_context.net.0 :: GELU
transformer.transformer_blocks.20.ff_context.net.0.proj :: Linear
transformer.transformer_blocks.20.ff_context.net.1 :: Dropout
transformer.transformer_blocks.20.ff_context.net.2 :: Linear
transformer.transformer_blocks.21 :: JointTransformerBlock
transformer.transformer_blocks.21.norm1 :: AdaLayerNormZero
transformer.transformer_blocks.21.norm1.silu :: SiLU
transformer.transformer_blocks.21.norm1.linear :: Linear
transformer.transformer_blocks.21.norm1.norm :: LayerNorm
transformer.transformer_blocks.21.norm1_context :: AdaLayerNormZero
transformer.transformer_blocks.21.norm1_context.silu :: SiLU
transformer.transformer_blocks.21.norm1_context.linear :: Linear
transformer.transformer_blocks.21.norm1_context.norm :: LayerNorm
transformer.transformer_blocks.21.attn :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.21.attn.norm_q :: RMSNorm
transformer.transformer_blocks.21.attn.norm_k :: RMSNorm
transformer.transformer_blocks.21.attn.to_q :: Linear
transformer.transformer_blocks.21.attn.to_k :: Linear
transformer.transformer_blocks.21.attn.to_v :: Linear
transformer.transformer_blocks.21.attn.add_k_proj :: Linear
transformer.transformer_blocks.21.attn.add_v_proj :: Linear
transformer.transformer_blocks.21.attn.add_q_proj :: Linear
transformer.transformer_blocks.21.attn.to_out :: ModuleList
transformer.transformer_blocks.21.attn.to_out.0 :: Linear
transformer.transformer_blocks.21.attn.to_out.1 :: Dropout
transformer.transformer_blocks.21.attn.to_add_out :: Linear
transformer.transformer_blocks.21.attn.norm_added_q :: RMSNorm
transformer.transformer_blocks.21.attn.norm_added_k :: RMSNorm
transformer.transformer_blocks.21.norm2 :: LayerNorm
transformer.transformer_blocks.21.ff :: FeedForward
transformer.transformer_blocks.21.ff.net :: ModuleList
transformer.transformer_blocks.21.ff.net.0 :: GELU
transformer.transformer_blocks.21.ff.net.0.proj :: Linear
transformer.transformer_blocks.21.ff.net.1 :: Dropout
transformer.transformer_blocks.21.ff.net.2 :: Linear
transformer.transformer_blocks.21.norm2_context :: LayerNorm
transformer.transformer_blocks.21.ff_context :: FeedForward
transformer.transformer_blocks.21.ff_context.net :: ModuleList
transformer.transformer_blocks.21.ff_context.net.0 :: GELU
transformer.transformer_blocks.21.ff_context.net.0.proj :: Linear
transformer.transformer_blocks.21.ff_context.net.1 :: Dropout
transformer.transformer_blocks.21.ff_context.net.2 :: Linear
transformer.transformer_blocks.22 :: JointTransformerBlock
transformer.transformer_blocks.22.norm1 :: AdaLayerNormZero
transformer.transformer_blocks.22.norm1.silu :: SiLU
transformer.transformer_blocks.22.norm1.linear :: Linear
transformer.transformer_blocks.22.norm1.norm :: LayerNorm
transformer.transformer_blocks.22.norm1_context :: AdaLayerNormZero
transformer.transformer_blocks.22.norm1_context.silu :: SiLU
transformer.transformer_blocks.22.norm1_context.linear :: Linear
transformer.transformer_blocks.22.norm1_context.norm :: LayerNorm
transformer.transformer_blocks.22.attn :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.22.attn.norm_q :: RMSNorm
transformer.transformer_blocks.22.attn.norm_k :: RMSNorm
transformer.transformer_blocks.22.attn.to_q :: Linear
transformer.transformer_blocks.22.attn.to_k :: Linear
transformer.transformer_blocks.22.attn.to_v :: Linear
transformer.transformer_blocks.22.attn.add_k_proj :: Linear
transformer.transformer_blocks.22.attn.add_v_proj :: Linear
transformer.transformer_blocks.22.attn.add_q_proj :: Linear
transformer.transformer_blocks.22.attn.to_out :: ModuleList
transformer.transformer_blocks.22.attn.to_out.0 :: Linear
transformer.transformer_blocks.22.attn.to_out.1 :: Dropout
transformer.transformer_blocks.22.attn.to_add_out :: Linear
transformer.transformer_blocks.22.attn.norm_added_q :: RMSNorm
transformer.transformer_blocks.22.attn.norm_added_k :: RMSNorm
transformer.transformer_blocks.22.norm2 :: LayerNorm
transformer.transformer_blocks.22.ff :: FeedForward
transformer.transformer_blocks.22.ff.net :: ModuleList
transformer.transformer_blocks.22.ff.net.0 :: GELU
transformer.transformer_blocks.22.ff.net.0.proj :: Linear
transformer.transformer_blocks.22.ff.net.1 :: Dropout
transformer.transformer_blocks.22.ff.net.2 :: Linear
transformer.transformer_blocks.22.norm2_context :: LayerNorm
transformer.transformer_blocks.22.ff_context :: FeedForward
transformer.transformer_blocks.22.ff_context.net :: ModuleList
transformer.transformer_blocks.22.ff_context.net.0 :: GELU
transformer.transformer_blocks.22.ff_context.net.0.proj :: Linear
transformer.transformer_blocks.22.ff_context.net.1 :: Dropout
transformer.transformer_blocks.22.ff_context.net.2 :: Linear
transformer.transformer_blocks.23 :: JointTransformerBlock
transformer.transformer_blocks.23.norm1 :: AdaLayerNormZero
transformer.transformer_blocks.23.norm1.silu :: SiLU
transformer.transformer_blocks.23.norm1.linear :: Linear
transformer.transformer_blocks.23.norm1.norm :: LayerNorm
transformer.transformer_blocks.23.norm1_context :: AdaLayerNormContinuous
transformer.transformer_blocks.23.norm1_context.silu :: SiLU
transformer.transformer_blocks.23.norm1_context.linear :: Linear
transformer.transformer_blocks.23.norm1_context.norm :: LayerNorm
transformer.transformer_blocks.23.attn :: Attention [QKV]
  - to_q  : W=(1536, 1536)  B=(1536,)
  - to_k  : W=(1536, 1536)  B=(1536,)
  - to_v  : W=(1536, 1536)  B=(1536,)
  - to_out: W=None  B=None
transformer.transformer_blocks.23.attn.norm_q :: RMSNorm
transformer.transformer_blocks.23.attn.norm_k :: RMSNorm
transformer.transformer_blocks.23.attn.to_q :: Linear
transformer.transformer_blocks.23.attn.to_k :: Linear
transformer.transformer_blocks.23.attn.to_v :: Linear
transformer.transformer_blocks.23.attn.add_k_proj :: Linear
transformer.transformer_blocks.23.attn.add_v_proj :: Linear
transformer.transformer_blocks.23.attn.add_q_proj :: Linear
transformer.transformer_blocks.23.attn.to_out :: ModuleList
transformer.transformer_blocks.23.attn.to_out.0 :: Linear
transformer.transformer_blocks.23.attn.to_out.1 :: Dropout
transformer.transformer_blocks.23.attn.norm_added_q :: RMSNorm
transformer.transformer_blocks.23.attn.norm_added_k :: RMSNorm
transformer.transformer_blocks.23.norm2 :: LayerNorm
transformer.transformer_blocks.23.ff :: FeedForward
transformer.transformer_blocks.23.ff.net :: ModuleList
transformer.transformer_blocks.23.ff.net.0 :: GELU
transformer.transformer_blocks.23.ff.net.0.proj :: Linear
transformer.transformer_blocks.23.ff.net.1 :: Dropout
transformer.transformer_blocks.23.ff.net.2 :: Linear
transformer.norm_out :: AdaLayerNormContinuous
transformer.norm_out.silu :: SiLU
transformer.norm_out.linear :: Linear
transformer.norm_out.norm :: LayerNorm
transformer.proj_out :: Linear
vae :: AutoencoderKL
vae.encoder :: Encoder [CROSS]
vae.encoder.conv_in :: Conv2d [CROSS]
vae.encoder.down_blocks :: ModuleList [CROSS]
vae.encoder.down_blocks.0 :: DownEncoderBlock2D [CROSS]
vae.encoder.down_blocks.0.resnets :: ModuleList [CROSS]
vae.encoder.down_blocks.0.resnets.0 :: ResnetBlock2D [CROSS]
vae.encoder.down_blocks.0.resnets.0.norm1 :: GroupNorm [CROSS]
vae.encoder.down_blocks.0.resnets.0.conv1 :: Conv2d [CROSS]
vae.encoder.down_blocks.0.resnets.0.norm2 :: GroupNorm [CROSS]
vae.encoder.down_blocks.0.resnets.0.dropout :: Dropout [CROSS]
vae.encoder.down_blocks.0.resnets.0.conv2 :: Conv2d [CROSS]
vae.encoder.down_blocks.0.resnets.0.nonlinearity :: SiLU [CROSS]
vae.encoder.down_blocks.0.resnets.1 :: ResnetBlock2D [CROSS]
vae.encoder.down_blocks.0.resnets.1.norm1 :: GroupNorm [CROSS]
vae.encoder.down_blocks.0.resnets.1.conv1 :: Conv2d [CROSS]
vae.encoder.down_blocks.0.resnets.1.norm2 :: GroupNorm [CROSS]
vae.encoder.down_blocks.0.resnets.1.dropout :: Dropout [CROSS]
vae.encoder.down_blocks.0.resnets.1.conv2 :: Conv2d [CROSS]
vae.encoder.down_blocks.0.resnets.1.nonlinearity :: SiLU [CROSS]
vae.encoder.down_blocks.0.downsamplers :: ModuleList [CROSS]
vae.encoder.down_blocks.0.downsamplers.0 :: Downsample2D [CROSS]
vae.encoder.down_blocks.0.downsamplers.0.conv :: Conv2d [CROSS]
vae.encoder.down_blocks.1 :: DownEncoderBlock2D [CROSS]
vae.encoder.down_blocks.1.resnets :: ModuleList [CROSS]
vae.encoder.down_blocks.1.resnets.0 :: ResnetBlock2D [CROSS]
vae.encoder.down_blocks.1.resnets.0.norm1 :: GroupNorm [CROSS]
vae.encoder.down_blocks.1.resnets.0.conv1 :: Conv2d [CROSS]
vae.encoder.down_blocks.1.resnets.0.norm2 :: GroupNorm [CROSS]
vae.encoder.down_blocks.1.resnets.0.dropout :: Dropout [CROSS]
vae.encoder.down_blocks.1.resnets.0.conv2 :: Conv2d [CROSS]
vae.encoder.down_blocks.1.resnets.0.nonlinearity :: SiLU [CROSS]
vae.encoder.down_blocks.1.resnets.0.conv_shortcut :: Conv2d [CROSS]
vae.encoder.down_blocks.1.resnets.1 :: ResnetBlock2D [CROSS]
vae.encoder.down_blocks.1.resnets.1.norm1 :: GroupNorm [CROSS]
vae.encoder.down_blocks.1.resnets.1.conv1 :: Conv2d [CROSS]
vae.encoder.down_blocks.1.resnets.1.norm2 :: GroupNorm [CROSS]
vae.encoder.down_blocks.1.resnets.1.dropout :: Dropout [CROSS]
vae.encoder.down_blocks.1.resnets.1.conv2 :: Conv2d [CROSS]
vae.encoder.down_blocks.1.resnets.1.nonlinearity :: SiLU [CROSS]
vae.encoder.down_blocks.1.downsamplers :: ModuleList [CROSS]
vae.encoder.down_blocks.1.downsamplers.0 :: Downsample2D [CROSS]
vae.encoder.down_blocks.1.downsamplers.0.conv :: Conv2d [CROSS]
vae.encoder.down_blocks.2 :: DownEncoderBlock2D [CROSS]
vae.encoder.down_blocks.2.resnets :: ModuleList [CROSS]
vae.encoder.down_blocks.2.resnets.0 :: ResnetBlock2D [CROSS]
vae.encoder.down_blocks.2.resnets.0.norm1 :: GroupNorm [CROSS]
vae.encoder.down_blocks.2.resnets.0.conv1 :: Conv2d [CROSS]
vae.encoder.down_blocks.2.resnets.0.norm2 :: GroupNorm [CROSS]
vae.encoder.down_blocks.2.resnets.0.dropout :: Dropout [CROSS]
vae.encoder.down_blocks.2.resnets.0.conv2 :: Conv2d [CROSS]
vae.encoder.down_blocks.2.resnets.0.nonlinearity :: SiLU [CROSS]
vae.encoder.down_blocks.2.resnets.0.conv_shortcut :: Conv2d [CROSS]
vae.encoder.down_blocks.2.resnets.1 :: ResnetBlock2D [CROSS]
vae.encoder.down_blocks.2.resnets.1.norm1 :: GroupNorm [CROSS]
vae.encoder.down_blocks.2.resnets.1.conv1 :: Conv2d [CROSS]
vae.encoder.down_blocks.2.resnets.1.norm2 :: GroupNorm [CROSS]
vae.encoder.down_blocks.2.resnets.1.dropout :: Dropout [CROSS]
vae.encoder.down_blocks.2.resnets.1.conv2 :: Conv2d [CROSS]
vae.encoder.down_blocks.2.resnets.1.nonlinearity :: SiLU [CROSS]
vae.encoder.down_blocks.2.downsamplers :: ModuleList [CROSS]
vae.encoder.down_blocks.2.downsamplers.0 :: Downsample2D [CROSS]
vae.encoder.down_blocks.2.downsamplers.0.conv :: Conv2d [CROSS]
vae.encoder.down_blocks.3 :: DownEncoderBlock2D [CROSS]
vae.encoder.down_blocks.3.resnets :: ModuleList [CROSS]
vae.encoder.down_blocks.3.resnets.0 :: ResnetBlock2D [CROSS]
vae.encoder.down_blocks.3.resnets.0.norm1 :: GroupNorm [CROSS]
vae.encoder.down_blocks.3.resnets.0.conv1 :: Conv2d [CROSS]
vae.encoder.down_blocks.3.resnets.0.norm2 :: GroupNorm [CROSS]
vae.encoder.down_blocks.3.resnets.0.dropout :: Dropout [CROSS]
vae.encoder.down_blocks.3.resnets.0.conv2 :: Conv2d [CROSS]
vae.encoder.down_blocks.3.resnets.0.nonlinearity :: SiLU [CROSS]
vae.encoder.down_blocks.3.resnets.1 :: ResnetBlock2D [CROSS]
vae.encoder.down_blocks.3.resnets.1.norm1 :: GroupNorm [CROSS]
vae.encoder.down_blocks.3.resnets.1.conv1 :: Conv2d [CROSS]
vae.encoder.down_blocks.3.resnets.1.norm2 :: GroupNorm [CROSS]
vae.encoder.down_blocks.3.resnets.1.dropout :: Dropout [CROSS]
vae.encoder.down_blocks.3.resnets.1.conv2 :: Conv2d [CROSS]
vae.encoder.down_blocks.3.resnets.1.nonlinearity :: SiLU [CROSS]
vae.encoder.mid_block :: UNetMidBlock2D [CROSS]
vae.encoder.mid_block.attentions :: ModuleList [CROSS]
vae.encoder.mid_block.attentions.0 :: Attention [QKV]
  - to_q  : W=(512, 512)  B=(512,)
  - to_k  : W=(512, 512)  B=(512,)
  - to_v  : W=(512, 512)  B=(512,)
  - to_out: W=None  B=None
vae.encoder.mid_block.attentions.0.group_norm :: GroupNorm [CROSS]
vae.encoder.mid_block.attentions.0.to_q :: Linear [CROSS]
vae.encoder.mid_block.attentions.0.to_k :: Linear [CROSS]
vae.encoder.mid_block.attentions.0.to_v :: Linear [CROSS]
vae.encoder.mid_block.attentions.0.to_out :: ModuleList [CROSS]
vae.encoder.mid_block.attentions.0.to_out.0 :: Linear [CROSS]
vae.encoder.mid_block.attentions.0.to_out.1 :: Dropout [CROSS]
vae.encoder.mid_block.resnets :: ModuleList [CROSS]
vae.encoder.mid_block.resnets.0 :: ResnetBlock2D [CROSS]
vae.encoder.mid_block.resnets.0.norm1 :: GroupNorm [CROSS]
vae.encoder.mid_block.resnets.0.conv1 :: Conv2d [CROSS]
vae.encoder.mid_block.resnets.0.norm2 :: GroupNorm [CROSS]
vae.encoder.mid_block.resnets.0.dropout :: Dropout [CROSS]
vae.encoder.mid_block.resnets.0.conv2 :: Conv2d [CROSS]
vae.encoder.mid_block.resnets.0.nonlinearity :: SiLU [CROSS]
vae.encoder.mid_block.resnets.1 :: ResnetBlock2D [CROSS]
vae.encoder.mid_block.resnets.1.norm1 :: GroupNorm [CROSS]
vae.encoder.mid_block.resnets.1.conv1 :: Conv2d [CROSS]
vae.encoder.mid_block.resnets.1.norm2 :: GroupNorm [CROSS]
vae.encoder.mid_block.resnets.1.dropout :: Dropout [CROSS]
vae.encoder.mid_block.resnets.1.conv2 :: Conv2d [CROSS]
vae.encoder.mid_block.resnets.1.nonlinearity :: SiLU [CROSS]
vae.encoder.conv_norm_out :: GroupNorm [CROSS]
vae.encoder.conv_act :: SiLU [CROSS]
vae.encoder.conv_out :: Conv2d [CROSS]
vae.decoder :: Decoder
vae.decoder.conv_in :: Conv2d
vae.decoder.up_blocks :: ModuleList
vae.decoder.up_blocks.0 :: UpDecoderBlock2D
vae.decoder.up_blocks.0.resnets :: ModuleList
vae.decoder.up_blocks.0.resnets.0 :: ResnetBlock2D
vae.decoder.up_blocks.0.resnets.0.norm1 :: GroupNorm
vae.decoder.up_blocks.0.resnets.0.conv1 :: Conv2d
vae.decoder.up_blocks.0.resnets.0.norm2 :: GroupNorm
vae.decoder.up_blocks.0.resnets.0.dropout :: Dropout
vae.decoder.up_blocks.0.resnets.0.conv2 :: Conv2d
vae.decoder.up_blocks.0.resnets.0.nonlinearity :: SiLU
vae.decoder.up_blocks.0.resnets.1 :: ResnetBlock2D
vae.decoder.up_blocks.0.resnets.1.norm1 :: GroupNorm
vae.decoder.up_blocks.0.resnets.1.conv1 :: Conv2d
vae.decoder.up_blocks.0.resnets.1.norm2 :: GroupNorm
vae.decoder.up_blocks.0.resnets.1.dropout :: Dropout
vae.decoder.up_blocks.0.resnets.1.conv2 :: Conv2d
vae.decoder.up_blocks.0.resnets.1.nonlinearity :: SiLU
vae.decoder.up_blocks.0.resnets.2 :: ResnetBlock2D
vae.decoder.up_blocks.0.resnets.2.norm1 :: GroupNorm
vae.decoder.up_blocks.0.resnets.2.conv1 :: Conv2d
vae.decoder.up_blocks.0.resnets.2.norm2 :: GroupNorm
vae.decoder.up_blocks.0.resnets.2.dropout :: Dropout
vae.decoder.up_blocks.0.resnets.2.conv2 :: Conv2d
vae.decoder.up_blocks.0.resnets.2.nonlinearity :: SiLU
vae.decoder.up_blocks.0.upsamplers :: ModuleList
vae.decoder.up_blocks.0.upsamplers.0 :: Upsample2D
vae.decoder.up_blocks.0.upsamplers.0.conv :: Conv2d
vae.decoder.up_blocks.1 :: UpDecoderBlock2D
vae.decoder.up_blocks.1.resnets :: ModuleList
vae.decoder.up_blocks.1.resnets.0 :: ResnetBlock2D
vae.decoder.up_blocks.1.resnets.0.norm1 :: GroupNorm
vae.decoder.up_blocks.1.resnets.0.conv1 :: Conv2d
vae.decoder.up_blocks.1.resnets.0.norm2 :: GroupNorm
vae.decoder.up_blocks.1.resnets.0.dropout :: Dropout
vae.decoder.up_blocks.1.resnets.0.conv2 :: Conv2d
vae.decoder.up_blocks.1.resnets.0.nonlinearity :: SiLU
vae.decoder.up_blocks.1.resnets.1 :: ResnetBlock2D
vae.decoder.up_blocks.1.resnets.1.norm1 :: GroupNorm
vae.decoder.up_blocks.1.resnets.1.conv1 :: Conv2d
vae.decoder.up_blocks.1.resnets.1.norm2 :: GroupNorm
vae.decoder.up_blocks.1.resnets.1.dropout :: Dropout
vae.decoder.up_blocks.1.resnets.1.conv2 :: Conv2d
vae.decoder.up_blocks.1.resnets.1.nonlinearity :: SiLU
vae.decoder.up_blocks.1.resnets.2 :: ResnetBlock2D
vae.decoder.up_blocks.1.resnets.2.norm1 :: GroupNorm
vae.decoder.up_blocks.1.resnets.2.conv1 :: Conv2d
vae.decoder.up_blocks.1.resnets.2.norm2 :: GroupNorm
vae.decoder.up_blocks.1.resnets.2.dropout :: Dropout
vae.decoder.up_blocks.1.resnets.2.conv2 :: Conv2d
vae.decoder.up_blocks.1.resnets.2.nonlinearity :: SiLU
vae.decoder.up_blocks.1.upsamplers :: ModuleList
vae.decoder.up_blocks.1.upsamplers.0 :: Upsample2D
vae.decoder.up_blocks.1.upsamplers.0.conv :: Conv2d
vae.decoder.up_blocks.2 :: UpDecoderBlock2D
vae.decoder.up_blocks.2.resnets :: ModuleList
vae.decoder.up_blocks.2.resnets.0 :: ResnetBlock2D
vae.decoder.up_blocks.2.resnets.0.norm1 :: GroupNorm
vae.decoder.up_blocks.2.resnets.0.conv1 :: Conv2d
vae.decoder.up_blocks.2.resnets.0.norm2 :: GroupNorm
vae.decoder.up_blocks.2.resnets.0.dropout :: Dropout
vae.decoder.up_blocks.2.resnets.0.conv2 :: Conv2d
vae.decoder.up_blocks.2.resnets.0.nonlinearity :: SiLU
vae.decoder.up_blocks.2.resnets.0.conv_shortcut :: Conv2d
vae.decoder.up_blocks.2.resnets.1 :: ResnetBlock2D
vae.decoder.up_blocks.2.resnets.1.norm1 :: GroupNorm
vae.decoder.up_blocks.2.resnets.1.conv1 :: Conv2d
vae.decoder.up_blocks.2.resnets.1.norm2 :: GroupNorm
vae.decoder.up_blocks.2.resnets.1.dropout :: Dropout
vae.decoder.up_blocks.2.resnets.1.conv2 :: Conv2d
vae.decoder.up_blocks.2.resnets.1.nonlinearity :: SiLU
vae.decoder.up_blocks.2.resnets.2 :: ResnetBlock2D
vae.decoder.up_blocks.2.resnets.2.norm1 :: GroupNorm
vae.decoder.up_blocks.2.resnets.2.conv1 :: Conv2d
vae.decoder.up_blocks.2.resnets.2.norm2 :: GroupNorm
vae.decoder.up_blocks.2.resnets.2.dropout :: Dropout
vae.decoder.up_blocks.2.resnets.2.conv2 :: Conv2d
vae.decoder.up_blocks.2.resnets.2.nonlinearity :: SiLU
vae.decoder.up_blocks.2.upsamplers :: ModuleList
vae.decoder.up_blocks.2.upsamplers.0 :: Upsample2D
vae.decoder.up_blocks.2.upsamplers.0.conv :: Conv2d
vae.decoder.up_blocks.3 :: UpDecoderBlock2D
vae.decoder.up_blocks.3.resnets :: ModuleList
vae.decoder.up_blocks.3.resnets.0 :: ResnetBlock2D
vae.decoder.up_blocks.3.resnets.0.norm1 :: GroupNorm
vae.decoder.up_blocks.3.resnets.0.conv1 :: Conv2d
vae.decoder.up_blocks.3.resnets.0.norm2 :: GroupNorm
vae.decoder.up_blocks.3.resnets.0.dropout :: Dropout
vae.decoder.up_blocks.3.resnets.0.conv2 :: Conv2d
vae.decoder.up_blocks.3.resnets.0.nonlinearity :: SiLU
vae.decoder.up_blocks.3.resnets.0.conv_shortcut :: Conv2d
vae.decoder.up_blocks.3.resnets.1 :: ResnetBlock2D
vae.decoder.up_blocks.3.resnets.1.norm1 :: GroupNorm
vae.decoder.up_blocks.3.resnets.1.conv1 :: Conv2d
vae.decoder.up_blocks.3.resnets.1.norm2 :: GroupNorm
vae.decoder.up_blocks.3.resnets.1.dropout :: Dropout
vae.decoder.up_blocks.3.resnets.1.conv2 :: Conv2d
vae.decoder.up_blocks.3.resnets.1.nonlinearity :: SiLU
vae.decoder.up_blocks.3.resnets.2 :: ResnetBlock2D
vae.decoder.up_blocks.3.resnets.2.norm1 :: GroupNorm
vae.decoder.up_blocks.3.resnets.2.conv1 :: Conv2d
vae.decoder.up_blocks.3.resnets.2.norm2 :: GroupNorm
vae.decoder.up_blocks.3.resnets.2.dropout :: Dropout
vae.decoder.up_blocks.3.resnets.2.conv2 :: Conv2d
vae.decoder.up_blocks.3.resnets.2.nonlinearity :: SiLU
vae.decoder.mid_block :: UNetMidBlock2D
vae.decoder.mid_block.attentions :: ModuleList
vae.decoder.mid_block.attentions.0 :: Attention [QKV]
  - to_q  : W=(512, 512)  B=(512,)
  - to_k  : W=(512, 512)  B=(512,)
  - to_v  : W=(512, 512)  B=(512,)
  - to_out: W=None  B=None
vae.decoder.mid_block.attentions.0.group_norm :: GroupNorm
vae.decoder.mid_block.attentions.0.to_q :: Linear
vae.decoder.mid_block.attentions.0.to_k :: Linear
vae.decoder.mid_block.attentions.0.to_v :: Linear
vae.decoder.mid_block.attentions.0.to_out :: ModuleList
vae.decoder.mid_block.attentions.0.to_out.0 :: Linear
vae.decoder.mid_block.attentions.0.to_out.1 :: Dropout
vae.decoder.mid_block.resnets :: ModuleList
vae.decoder.mid_block.resnets.0 :: ResnetBlock2D
vae.decoder.mid_block.resnets.0.norm1 :: GroupNorm
vae.decoder.mid_block.resnets.0.conv1 :: Conv2d
vae.decoder.mid_block.resnets.0.norm2 :: GroupNorm
vae.decoder.mid_block.resnets.0.dropout :: Dropout
vae.decoder.mid_block.resnets.0.conv2 :: Conv2d
vae.decoder.mid_block.resnets.0.nonlinearity :: SiLU
vae.decoder.mid_block.resnets.1 :: ResnetBlock2D
vae.decoder.mid_block.resnets.1.norm1 :: GroupNorm
vae.decoder.mid_block.resnets.1.conv1 :: Conv2d
vae.decoder.mid_block.resnets.1.norm2 :: GroupNorm
vae.decoder.mid_block.resnets.1.dropout :: Dropout
vae.decoder.mid_block.resnets.1.conv2 :: Conv2d
vae.decoder.mid_block.resnets.1.nonlinearity :: SiLU
vae.decoder.conv_norm_out :: GroupNorm
vae.decoder.conv_act :: SiLU
vae.decoder.conv_out :: Conv2d
text_encoder :: CLIPTextModelWithProjection [CROSS]
text_encoder.text_model :: CLIPTextTransformer [CROSS]
text_encoder.text_model.embeddings :: CLIPTextEmbeddings [CROSS]
text_encoder.text_model.embeddings.token_embedding :: Embedding [CROSS]
text_encoder.text_model.embeddings.position_embedding :: Embedding [CROSS]
text_encoder.text_model.encoder :: CLIPEncoder [CROSS]
text_encoder.text_model.encoder.layers :: ModuleList [CROSS]
text_encoder.text_model.encoder.layers.0 :: CLIPEncoderLayer [CROSS]
text_encoder.text_model.encoder.layers.0.self_attn :: CLIPAttention [CROSS]
text_encoder.text_model.encoder.layers.0.self_attn.k_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.0.self_attn.v_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.0.self_attn.q_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.0.self_attn.out_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.0.layer_norm1 :: LayerNorm [CROSS]
text_encoder.text_model.encoder.layers.0.mlp :: CLIPMLP [CROSS]
text_encoder.text_model.encoder.layers.0.mlp.activation_fn :: QuickGELUActivation [CROSS]
text_encoder.text_model.encoder.layers.0.mlp.fc1 :: Linear [CROSS]
text_encoder.text_model.encoder.layers.0.mlp.fc2 :: Linear [CROSS]
text_encoder.text_model.encoder.layers.0.layer_norm2 :: LayerNorm [CROSS]
text_encoder.text_model.encoder.layers.1 :: CLIPEncoderLayer [CROSS]
text_encoder.text_model.encoder.layers.1.self_attn :: CLIPAttention [CROSS]
text_encoder.text_model.encoder.layers.1.self_attn.k_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.1.self_attn.v_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.1.self_attn.q_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.1.self_attn.out_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.1.layer_norm1 :: LayerNorm [CROSS]
text_encoder.text_model.encoder.layers.1.mlp :: CLIPMLP [CROSS]
text_encoder.text_model.encoder.layers.1.mlp.activation_fn :: QuickGELUActivation [CROSS]
text_encoder.text_model.encoder.layers.1.mlp.fc1 :: Linear [CROSS]
text_encoder.text_model.encoder.layers.1.mlp.fc2 :: Linear [CROSS]
text_encoder.text_model.encoder.layers.1.layer_norm2 :: LayerNorm [CROSS]
text_encoder.text_model.encoder.layers.2 :: CLIPEncoderLayer [CROSS]
text_encoder.text_model.encoder.layers.2.self_attn :: CLIPAttention [CROSS]
text_encoder.text_model.encoder.layers.2.self_attn.k_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.2.self_attn.v_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.2.self_attn.q_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.2.self_attn.out_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.2.layer_norm1 :: LayerNorm [CROSS]
text_encoder.text_model.encoder.layers.2.mlp :: CLIPMLP [CROSS]
text_encoder.text_model.encoder.layers.2.mlp.activation_fn :: QuickGELUActivation [CROSS]
text_encoder.text_model.encoder.layers.2.mlp.fc1 :: Linear [CROSS]
text_encoder.text_model.encoder.layers.2.mlp.fc2 :: Linear [CROSS]
text_encoder.text_model.encoder.layers.2.layer_norm2 :: LayerNorm [CROSS]
text_encoder.text_model.encoder.layers.3 :: CLIPEncoderLayer [CROSS]
text_encoder.text_model.encoder.layers.3.self_attn :: CLIPAttention [CROSS]
text_encoder.text_model.encoder.layers.3.self_attn.k_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.3.self_attn.v_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.3.self_attn.q_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.3.self_attn.out_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.3.layer_norm1 :: LayerNorm [CROSS]
text_encoder.text_model.encoder.layers.3.mlp :: CLIPMLP [CROSS]
text_encoder.text_model.encoder.layers.3.mlp.activation_fn :: QuickGELUActivation [CROSS]
text_encoder.text_model.encoder.layers.3.mlp.fc1 :: Linear [CROSS]
text_encoder.text_model.encoder.layers.3.mlp.fc2 :: Linear [CROSS]
text_encoder.text_model.encoder.layers.3.layer_norm2 :: LayerNorm [CROSS]
text_encoder.text_model.encoder.layers.4 :: CLIPEncoderLayer [CROSS]
text_encoder.text_model.encoder.layers.4.self_attn :: CLIPAttention [CROSS]
text_encoder.text_model.encoder.layers.4.self_attn.k_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.4.self_attn.v_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.4.self_attn.q_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.4.self_attn.out_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.4.layer_norm1 :: LayerNorm [CROSS]
text_encoder.text_model.encoder.layers.4.mlp :: CLIPMLP [CROSS]
text_encoder.text_model.encoder.layers.4.mlp.activation_fn :: QuickGELUActivation [CROSS]
text_encoder.text_model.encoder.layers.4.mlp.fc1 :: Linear [CROSS]
text_encoder.text_model.encoder.layers.4.mlp.fc2 :: Linear [CROSS]
text_encoder.text_model.encoder.layers.4.layer_norm2 :: LayerNorm [CROSS]
text_encoder.text_model.encoder.layers.5 :: CLIPEncoderLayer [CROSS]
text_encoder.text_model.encoder.layers.5.self_attn :: CLIPAttention [CROSS]
text_encoder.text_model.encoder.layers.5.self_attn.k_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.5.self_attn.v_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.5.self_attn.q_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.5.self_attn.out_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.5.layer_norm1 :: LayerNorm [CROSS]
text_encoder.text_model.encoder.layers.5.mlp :: CLIPMLP [CROSS]
text_encoder.text_model.encoder.layers.5.mlp.activation_fn :: QuickGELUActivation [CROSS]
text_encoder.text_model.encoder.layers.5.mlp.fc1 :: Linear [CROSS]
text_encoder.text_model.encoder.layers.5.mlp.fc2 :: Linear [CROSS]
text_encoder.text_model.encoder.layers.5.layer_norm2 :: LayerNorm [CROSS]
text_encoder.text_model.encoder.layers.6 :: CLIPEncoderLayer [CROSS]
text_encoder.text_model.encoder.layers.6.self_attn :: CLIPAttention [CROSS]
text_encoder.text_model.encoder.layers.6.self_attn.k_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.6.self_attn.v_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.6.self_attn.q_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.6.self_attn.out_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.6.layer_norm1 :: LayerNorm [CROSS]
text_encoder.text_model.encoder.layers.6.mlp :: CLIPMLP [CROSS]
text_encoder.text_model.encoder.layers.6.mlp.activation_fn :: QuickGELUActivation [CROSS]
text_encoder.text_model.encoder.layers.6.mlp.fc1 :: Linear [CROSS]
text_encoder.text_model.encoder.layers.6.mlp.fc2 :: Linear [CROSS]
text_encoder.text_model.encoder.layers.6.layer_norm2 :: LayerNorm [CROSS]
text_encoder.text_model.encoder.layers.7 :: CLIPEncoderLayer [CROSS]
text_encoder.text_model.encoder.layers.7.self_attn :: CLIPAttention [CROSS]
text_encoder.text_model.encoder.layers.7.self_attn.k_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.7.self_attn.v_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.7.self_attn.q_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.7.self_attn.out_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.7.layer_norm1 :: LayerNorm [CROSS]
text_encoder.text_model.encoder.layers.7.mlp :: CLIPMLP [CROSS]
text_encoder.text_model.encoder.layers.7.mlp.activation_fn :: QuickGELUActivation [CROSS]
text_encoder.text_model.encoder.layers.7.mlp.fc1 :: Linear [CROSS]
text_encoder.text_model.encoder.layers.7.mlp.fc2 :: Linear [CROSS]
text_encoder.text_model.encoder.layers.7.layer_norm2 :: LayerNorm [CROSS]
text_encoder.text_model.encoder.layers.8 :: CLIPEncoderLayer [CROSS]
text_encoder.text_model.encoder.layers.8.self_attn :: CLIPAttention [CROSS]
text_encoder.text_model.encoder.layers.8.self_attn.k_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.8.self_attn.v_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.8.self_attn.q_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.8.self_attn.out_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.8.layer_norm1 :: LayerNorm [CROSS]
text_encoder.text_model.encoder.layers.8.mlp :: CLIPMLP [CROSS]
text_encoder.text_model.encoder.layers.8.mlp.activation_fn :: QuickGELUActivation [CROSS]
text_encoder.text_model.encoder.layers.8.mlp.fc1 :: Linear [CROSS]
text_encoder.text_model.encoder.layers.8.mlp.fc2 :: Linear [CROSS]
text_encoder.text_model.encoder.layers.8.layer_norm2 :: LayerNorm [CROSS]
text_encoder.text_model.encoder.layers.9 :: CLIPEncoderLayer [CROSS]
text_encoder.text_model.encoder.layers.9.self_attn :: CLIPAttention [CROSS]
text_encoder.text_model.encoder.layers.9.self_attn.k_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.9.self_attn.v_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.9.self_attn.q_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.9.self_attn.out_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.9.layer_norm1 :: LayerNorm [CROSS]
text_encoder.text_model.encoder.layers.9.mlp :: CLIPMLP [CROSS]
text_encoder.text_model.encoder.layers.9.mlp.activation_fn :: QuickGELUActivation [CROSS]
text_encoder.text_model.encoder.layers.9.mlp.fc1 :: Linear [CROSS]
text_encoder.text_model.encoder.layers.9.mlp.fc2 :: Linear [CROSS]
text_encoder.text_model.encoder.layers.9.layer_norm2 :: LayerNorm [CROSS]
text_encoder.text_model.encoder.layers.10 :: CLIPEncoderLayer [CROSS]
text_encoder.text_model.encoder.layers.10.self_attn :: CLIPAttention [CROSS]
text_encoder.text_model.encoder.layers.10.self_attn.k_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.10.self_attn.v_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.10.self_attn.q_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.10.self_attn.out_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.10.layer_norm1 :: LayerNorm [CROSS]
text_encoder.text_model.encoder.layers.10.mlp :: CLIPMLP [CROSS]
text_encoder.text_model.encoder.layers.10.mlp.activation_fn :: QuickGELUActivation [CROSS]
text_encoder.text_model.encoder.layers.10.mlp.fc1 :: Linear [CROSS]
text_encoder.text_model.encoder.layers.10.mlp.fc2 :: Linear [CROSS]
text_encoder.text_model.encoder.layers.10.layer_norm2 :: LayerNorm [CROSS]
text_encoder.text_model.encoder.layers.11 :: CLIPEncoderLayer [CROSS]
text_encoder.text_model.encoder.layers.11.self_attn :: CLIPAttention [CROSS]
text_encoder.text_model.encoder.layers.11.self_attn.k_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.11.self_attn.v_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.11.self_attn.q_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.11.self_attn.out_proj :: Linear [CROSS]
text_encoder.text_model.encoder.layers.11.layer_norm1 :: LayerNorm [CROSS]
text_encoder.text_model.encoder.layers.11.mlp :: CLIPMLP [CROSS]
text_encoder.text_model.encoder.layers.11.mlp.activation_fn :: QuickGELUActivation [CROSS]
text_encoder.text_model.encoder.layers.11.mlp.fc1 :: Linear [CROSS]
text_encoder.text_model.encoder.layers.11.mlp.fc2 :: Linear [CROSS]
text_encoder.text_model.encoder.layers.11.layer_norm2 :: LayerNorm [CROSS]
text_encoder.text_model.final_layer_norm :: LayerNorm [CROSS]
text_encoder.text_projection :: Linear [CROSS]
text_encoder_2 :: CLIPTextModelWithProjection [CROSS]
text_encoder_2.text_model :: CLIPTextTransformer [CROSS]
text_encoder_2.text_model.embeddings :: CLIPTextEmbeddings [CROSS]
text_encoder_2.text_model.embeddings.token_embedding :: Embedding [CROSS]
text_encoder_2.text_model.embeddings.position_embedding :: Embedding [CROSS]
text_encoder_2.text_model.encoder :: CLIPEncoder [CROSS]
text_encoder_2.text_model.encoder.layers :: ModuleList [CROSS]
text_encoder_2.text_model.encoder.layers.0 :: CLIPEncoderLayer [CROSS]
text_encoder_2.text_model.encoder.layers.0.self_attn :: CLIPAttention [CROSS]
text_encoder_2.text_model.encoder.layers.0.self_attn.k_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.0.self_attn.v_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.0.self_attn.q_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.0.self_attn.out_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.0.layer_norm1 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.0.mlp :: CLIPMLP [CROSS]
text_encoder_2.text_model.encoder.layers.0.mlp.activation_fn :: GELUActivation [CROSS]
text_encoder_2.text_model.encoder.layers.0.mlp.fc1 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.0.mlp.fc2 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.0.layer_norm2 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.1 :: CLIPEncoderLayer [CROSS]
text_encoder_2.text_model.encoder.layers.1.self_attn :: CLIPAttention [CROSS]
text_encoder_2.text_model.encoder.layers.1.self_attn.k_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.1.self_attn.v_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.1.self_attn.q_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.1.self_attn.out_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.1.layer_norm1 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.1.mlp :: CLIPMLP [CROSS]
text_encoder_2.text_model.encoder.layers.1.mlp.activation_fn :: GELUActivation [CROSS]
text_encoder_2.text_model.encoder.layers.1.mlp.fc1 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.1.mlp.fc2 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.1.layer_norm2 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.2 :: CLIPEncoderLayer [CROSS]
text_encoder_2.text_model.encoder.layers.2.self_attn :: CLIPAttention [CROSS]
text_encoder_2.text_model.encoder.layers.2.self_attn.k_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.2.self_attn.v_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.2.self_attn.q_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.2.self_attn.out_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.2.layer_norm1 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.2.mlp :: CLIPMLP [CROSS]
text_encoder_2.text_model.encoder.layers.2.mlp.activation_fn :: GELUActivation [CROSS]
text_encoder_2.text_model.encoder.layers.2.mlp.fc1 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.2.mlp.fc2 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.2.layer_norm2 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.3 :: CLIPEncoderLayer [CROSS]
text_encoder_2.text_model.encoder.layers.3.self_attn :: CLIPAttention [CROSS]
text_encoder_2.text_model.encoder.layers.3.self_attn.k_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.3.self_attn.v_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.3.self_attn.q_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.3.self_attn.out_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.3.layer_norm1 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.3.mlp :: CLIPMLP [CROSS]
text_encoder_2.text_model.encoder.layers.3.mlp.activation_fn :: GELUActivation [CROSS]
text_encoder_2.text_model.encoder.layers.3.mlp.fc1 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.3.mlp.fc2 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.3.layer_norm2 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.4 :: CLIPEncoderLayer [CROSS]
text_encoder_2.text_model.encoder.layers.4.self_attn :: CLIPAttention [CROSS]
text_encoder_2.text_model.encoder.layers.4.self_attn.k_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.4.self_attn.v_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.4.self_attn.q_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.4.self_attn.out_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.4.layer_norm1 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.4.mlp :: CLIPMLP [CROSS]
text_encoder_2.text_model.encoder.layers.4.mlp.activation_fn :: GELUActivation [CROSS]
text_encoder_2.text_model.encoder.layers.4.mlp.fc1 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.4.mlp.fc2 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.4.layer_norm2 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.5 :: CLIPEncoderLayer [CROSS]
text_encoder_2.text_model.encoder.layers.5.self_attn :: CLIPAttention [CROSS]
text_encoder_2.text_model.encoder.layers.5.self_attn.k_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.5.self_attn.v_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.5.self_attn.q_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.5.self_attn.out_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.5.layer_norm1 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.5.mlp :: CLIPMLP [CROSS]
text_encoder_2.text_model.encoder.layers.5.mlp.activation_fn :: GELUActivation [CROSS]
text_encoder_2.text_model.encoder.layers.5.mlp.fc1 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.5.mlp.fc2 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.5.layer_norm2 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.6 :: CLIPEncoderLayer [CROSS]
text_encoder_2.text_model.encoder.layers.6.self_attn :: CLIPAttention [CROSS]
text_encoder_2.text_model.encoder.layers.6.self_attn.k_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.6.self_attn.v_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.6.self_attn.q_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.6.self_attn.out_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.6.layer_norm1 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.6.mlp :: CLIPMLP [CROSS]
text_encoder_2.text_model.encoder.layers.6.mlp.activation_fn :: GELUActivation [CROSS]
text_encoder_2.text_model.encoder.layers.6.mlp.fc1 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.6.mlp.fc2 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.6.layer_norm2 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.7 :: CLIPEncoderLayer [CROSS]
text_encoder_2.text_model.encoder.layers.7.self_attn :: CLIPAttention [CROSS]
text_encoder_2.text_model.encoder.layers.7.self_attn.k_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.7.self_attn.v_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.7.self_attn.q_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.7.self_attn.out_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.7.layer_norm1 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.7.mlp :: CLIPMLP [CROSS]
text_encoder_2.text_model.encoder.layers.7.mlp.activation_fn :: GELUActivation [CROSS]
text_encoder_2.text_model.encoder.layers.7.mlp.fc1 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.7.mlp.fc2 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.7.layer_norm2 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.8 :: CLIPEncoderLayer [CROSS]
text_encoder_2.text_model.encoder.layers.8.self_attn :: CLIPAttention [CROSS]
text_encoder_2.text_model.encoder.layers.8.self_attn.k_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.8.self_attn.v_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.8.self_attn.q_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.8.self_attn.out_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.8.layer_norm1 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.8.mlp :: CLIPMLP [CROSS]
text_encoder_2.text_model.encoder.layers.8.mlp.activation_fn :: GELUActivation [CROSS]
text_encoder_2.text_model.encoder.layers.8.mlp.fc1 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.8.mlp.fc2 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.8.layer_norm2 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.9 :: CLIPEncoderLayer [CROSS]
text_encoder_2.text_model.encoder.layers.9.self_attn :: CLIPAttention [CROSS]
text_encoder_2.text_model.encoder.layers.9.self_attn.k_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.9.self_attn.v_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.9.self_attn.q_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.9.self_attn.out_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.9.layer_norm1 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.9.mlp :: CLIPMLP [CROSS]
text_encoder_2.text_model.encoder.layers.9.mlp.activation_fn :: GELUActivation [CROSS]
text_encoder_2.text_model.encoder.layers.9.mlp.fc1 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.9.mlp.fc2 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.9.layer_norm2 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.10 :: CLIPEncoderLayer [CROSS]
text_encoder_2.text_model.encoder.layers.10.self_attn :: CLIPAttention [CROSS]
text_encoder_2.text_model.encoder.layers.10.self_attn.k_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.10.self_attn.v_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.10.self_attn.q_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.10.self_attn.out_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.10.layer_norm1 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.10.mlp :: CLIPMLP [CROSS]
text_encoder_2.text_model.encoder.layers.10.mlp.activation_fn :: GELUActivation [CROSS]
text_encoder_2.text_model.encoder.layers.10.mlp.fc1 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.10.mlp.fc2 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.10.layer_norm2 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.11 :: CLIPEncoderLayer [CROSS]
text_encoder_2.text_model.encoder.layers.11.self_attn :: CLIPAttention [CROSS]
text_encoder_2.text_model.encoder.layers.11.self_attn.k_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.11.self_attn.v_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.11.self_attn.q_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.11.self_attn.out_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.11.layer_norm1 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.11.mlp :: CLIPMLP [CROSS]
text_encoder_2.text_model.encoder.layers.11.mlp.activation_fn :: GELUActivation [CROSS]
text_encoder_2.text_model.encoder.layers.11.mlp.fc1 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.11.mlp.fc2 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.11.layer_norm2 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.12 :: CLIPEncoderLayer [CROSS]
text_encoder_2.text_model.encoder.layers.12.self_attn :: CLIPAttention [CROSS]
text_encoder_2.text_model.encoder.layers.12.self_attn.k_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.12.self_attn.v_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.12.self_attn.q_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.12.self_attn.out_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.12.layer_norm1 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.12.mlp :: CLIPMLP [CROSS]
text_encoder_2.text_model.encoder.layers.12.mlp.activation_fn :: GELUActivation [CROSS]
text_encoder_2.text_model.encoder.layers.12.mlp.fc1 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.12.mlp.fc2 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.12.layer_norm2 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.13 :: CLIPEncoderLayer [CROSS]
text_encoder_2.text_model.encoder.layers.13.self_attn :: CLIPAttention [CROSS]
text_encoder_2.text_model.encoder.layers.13.self_attn.k_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.13.self_attn.v_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.13.self_attn.q_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.13.self_attn.out_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.13.layer_norm1 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.13.mlp :: CLIPMLP [CROSS]
text_encoder_2.text_model.encoder.layers.13.mlp.activation_fn :: GELUActivation [CROSS]
text_encoder_2.text_model.encoder.layers.13.mlp.fc1 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.13.mlp.fc2 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.13.layer_norm2 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.14 :: CLIPEncoderLayer [CROSS]
text_encoder_2.text_model.encoder.layers.14.self_attn :: CLIPAttention [CROSS]
text_encoder_2.text_model.encoder.layers.14.self_attn.k_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.14.self_attn.v_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.14.self_attn.q_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.14.self_attn.out_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.14.layer_norm1 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.14.mlp :: CLIPMLP [CROSS]
text_encoder_2.text_model.encoder.layers.14.mlp.activation_fn :: GELUActivation [CROSS]
text_encoder_2.text_model.encoder.layers.14.mlp.fc1 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.14.mlp.fc2 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.14.layer_norm2 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.15 :: CLIPEncoderLayer [CROSS]
text_encoder_2.text_model.encoder.layers.15.self_attn :: CLIPAttention [CROSS]
text_encoder_2.text_model.encoder.layers.15.self_attn.k_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.15.self_attn.v_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.15.self_attn.q_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.15.self_attn.out_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.15.layer_norm1 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.15.mlp :: CLIPMLP [CROSS]
text_encoder_2.text_model.encoder.layers.15.mlp.activation_fn :: GELUActivation [CROSS]
text_encoder_2.text_model.encoder.layers.15.mlp.fc1 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.15.mlp.fc2 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.15.layer_norm2 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.16 :: CLIPEncoderLayer [CROSS]
text_encoder_2.text_model.encoder.layers.16.self_attn :: CLIPAttention [CROSS]
text_encoder_2.text_model.encoder.layers.16.self_attn.k_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.16.self_attn.v_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.16.self_attn.q_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.16.self_attn.out_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.16.layer_norm1 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.16.mlp :: CLIPMLP [CROSS]
text_encoder_2.text_model.encoder.layers.16.mlp.activation_fn :: GELUActivation [CROSS]
text_encoder_2.text_model.encoder.layers.16.mlp.fc1 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.16.mlp.fc2 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.16.layer_norm2 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.17 :: CLIPEncoderLayer [CROSS]
text_encoder_2.text_model.encoder.layers.17.self_attn :: CLIPAttention [CROSS]
text_encoder_2.text_model.encoder.layers.17.self_attn.k_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.17.self_attn.v_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.17.self_attn.q_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.17.self_attn.out_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.17.layer_norm1 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.17.mlp :: CLIPMLP [CROSS]
text_encoder_2.text_model.encoder.layers.17.mlp.activation_fn :: GELUActivation [CROSS]
text_encoder_2.text_model.encoder.layers.17.mlp.fc1 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.17.mlp.fc2 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.17.layer_norm2 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.18 :: CLIPEncoderLayer [CROSS]
text_encoder_2.text_model.encoder.layers.18.self_attn :: CLIPAttention [CROSS]
text_encoder_2.text_model.encoder.layers.18.self_attn.k_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.18.self_attn.v_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.18.self_attn.q_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.18.self_attn.out_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.18.layer_norm1 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.18.mlp :: CLIPMLP [CROSS]
text_encoder_2.text_model.encoder.layers.18.mlp.activation_fn :: GELUActivation [CROSS]
text_encoder_2.text_model.encoder.layers.18.mlp.fc1 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.18.mlp.fc2 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.18.layer_norm2 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.19 :: CLIPEncoderLayer [CROSS]
text_encoder_2.text_model.encoder.layers.19.self_attn :: CLIPAttention [CROSS]
text_encoder_2.text_model.encoder.layers.19.self_attn.k_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.19.self_attn.v_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.19.self_attn.q_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.19.self_attn.out_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.19.layer_norm1 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.19.mlp :: CLIPMLP [CROSS]
text_encoder_2.text_model.encoder.layers.19.mlp.activation_fn :: GELUActivation [CROSS]
text_encoder_2.text_model.encoder.layers.19.mlp.fc1 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.19.mlp.fc2 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.19.layer_norm2 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.20 :: CLIPEncoderLayer [CROSS]
text_encoder_2.text_model.encoder.layers.20.self_attn :: CLIPAttention [CROSS]
text_encoder_2.text_model.encoder.layers.20.self_attn.k_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.20.self_attn.v_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.20.self_attn.q_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.20.self_attn.out_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.20.layer_norm1 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.20.mlp :: CLIPMLP [CROSS]
text_encoder_2.text_model.encoder.layers.20.mlp.activation_fn :: GELUActivation [CROSS]
text_encoder_2.text_model.encoder.layers.20.mlp.fc1 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.20.mlp.fc2 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.20.layer_norm2 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.21 :: CLIPEncoderLayer [CROSS]
text_encoder_2.text_model.encoder.layers.21.self_attn :: CLIPAttention [CROSS]
text_encoder_2.text_model.encoder.layers.21.self_attn.k_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.21.self_attn.v_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.21.self_attn.q_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.21.self_attn.out_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.21.layer_norm1 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.21.mlp :: CLIPMLP [CROSS]
text_encoder_2.text_model.encoder.layers.21.mlp.activation_fn :: GELUActivation [CROSS]
text_encoder_2.text_model.encoder.layers.21.mlp.fc1 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.21.mlp.fc2 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.21.layer_norm2 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.22 :: CLIPEncoderLayer [CROSS]
text_encoder_2.text_model.encoder.layers.22.self_attn :: CLIPAttention [CROSS]
text_encoder_2.text_model.encoder.layers.22.self_attn.k_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.22.self_attn.v_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.22.self_attn.q_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.22.self_attn.out_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.22.layer_norm1 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.22.mlp :: CLIPMLP [CROSS]
text_encoder_2.text_model.encoder.layers.22.mlp.activation_fn :: GELUActivation [CROSS]
text_encoder_2.text_model.encoder.layers.22.mlp.fc1 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.22.mlp.fc2 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.22.layer_norm2 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.23 :: CLIPEncoderLayer [CROSS]
text_encoder_2.text_model.encoder.layers.23.self_attn :: CLIPAttention [CROSS]
text_encoder_2.text_model.encoder.layers.23.self_attn.k_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.23.self_attn.v_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.23.self_attn.q_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.23.self_attn.out_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.23.layer_norm1 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.23.mlp :: CLIPMLP [CROSS]
text_encoder_2.text_model.encoder.layers.23.mlp.activation_fn :: GELUActivation [CROSS]
text_encoder_2.text_model.encoder.layers.23.mlp.fc1 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.23.mlp.fc2 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.23.layer_norm2 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.24 :: CLIPEncoderLayer [CROSS]
text_encoder_2.text_model.encoder.layers.24.self_attn :: CLIPAttention [CROSS]
text_encoder_2.text_model.encoder.layers.24.self_attn.k_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.24.self_attn.v_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.24.self_attn.q_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.24.self_attn.out_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.24.layer_norm1 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.24.mlp :: CLIPMLP [CROSS]
text_encoder_2.text_model.encoder.layers.24.mlp.activation_fn :: GELUActivation [CROSS]
text_encoder_2.text_model.encoder.layers.24.mlp.fc1 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.24.mlp.fc2 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.24.layer_norm2 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.25 :: CLIPEncoderLayer [CROSS]
text_encoder_2.text_model.encoder.layers.25.self_attn :: CLIPAttention [CROSS]
text_encoder_2.text_model.encoder.layers.25.self_attn.k_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.25.self_attn.v_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.25.self_attn.q_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.25.self_attn.out_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.25.layer_norm1 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.25.mlp :: CLIPMLP [CROSS]
text_encoder_2.text_model.encoder.layers.25.mlp.activation_fn :: GELUActivation [CROSS]
text_encoder_2.text_model.encoder.layers.25.mlp.fc1 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.25.mlp.fc2 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.25.layer_norm2 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.26 :: CLIPEncoderLayer [CROSS]
text_encoder_2.text_model.encoder.layers.26.self_attn :: CLIPAttention [CROSS]
text_encoder_2.text_model.encoder.layers.26.self_attn.k_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.26.self_attn.v_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.26.self_attn.q_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.26.self_attn.out_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.26.layer_norm1 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.26.mlp :: CLIPMLP [CROSS]
text_encoder_2.text_model.encoder.layers.26.mlp.activation_fn :: GELUActivation [CROSS]
text_encoder_2.text_model.encoder.layers.26.mlp.fc1 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.26.mlp.fc2 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.26.layer_norm2 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.27 :: CLIPEncoderLayer [CROSS]
text_encoder_2.text_model.encoder.layers.27.self_attn :: CLIPAttention [CROSS]
text_encoder_2.text_model.encoder.layers.27.self_attn.k_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.27.self_attn.v_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.27.self_attn.q_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.27.self_attn.out_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.27.layer_norm1 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.27.mlp :: CLIPMLP [CROSS]
text_encoder_2.text_model.encoder.layers.27.mlp.activation_fn :: GELUActivation [CROSS]
text_encoder_2.text_model.encoder.layers.27.mlp.fc1 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.27.mlp.fc2 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.27.layer_norm2 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.28 :: CLIPEncoderLayer [CROSS]
text_encoder_2.text_model.encoder.layers.28.self_attn :: CLIPAttention [CROSS]
text_encoder_2.text_model.encoder.layers.28.self_attn.k_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.28.self_attn.v_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.28.self_attn.q_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.28.self_attn.out_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.28.layer_norm1 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.28.mlp :: CLIPMLP [CROSS]
text_encoder_2.text_model.encoder.layers.28.mlp.activation_fn :: GELUActivation [CROSS]
text_encoder_2.text_model.encoder.layers.28.mlp.fc1 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.28.mlp.fc2 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.28.layer_norm2 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.29 :: CLIPEncoderLayer [CROSS]
text_encoder_2.text_model.encoder.layers.29.self_attn :: CLIPAttention [CROSS]
text_encoder_2.text_model.encoder.layers.29.self_attn.k_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.29.self_attn.v_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.29.self_attn.q_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.29.self_attn.out_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.29.layer_norm1 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.29.mlp :: CLIPMLP [CROSS]
text_encoder_2.text_model.encoder.layers.29.mlp.activation_fn :: GELUActivation [CROSS]
text_encoder_2.text_model.encoder.layers.29.mlp.fc1 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.29.mlp.fc2 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.29.layer_norm2 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.30 :: CLIPEncoderLayer [CROSS]
text_encoder_2.text_model.encoder.layers.30.self_attn :: CLIPAttention [CROSS]
text_encoder_2.text_model.encoder.layers.30.self_attn.k_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.30.self_attn.v_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.30.self_attn.q_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.30.self_attn.out_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.30.layer_norm1 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.30.mlp :: CLIPMLP [CROSS]
text_encoder_2.text_model.encoder.layers.30.mlp.activation_fn :: GELUActivation [CROSS]
text_encoder_2.text_model.encoder.layers.30.mlp.fc1 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.30.mlp.fc2 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.30.layer_norm2 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.31 :: CLIPEncoderLayer [CROSS]
text_encoder_2.text_model.encoder.layers.31.self_attn :: CLIPAttention [CROSS]
text_encoder_2.text_model.encoder.layers.31.self_attn.k_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.31.self_attn.v_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.31.self_attn.q_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.31.self_attn.out_proj :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.31.layer_norm1 :: LayerNorm [CROSS]
text_encoder_2.text_model.encoder.layers.31.mlp :: CLIPMLP [CROSS]
text_encoder_2.text_model.encoder.layers.31.mlp.activation_fn :: GELUActivation [CROSS]
text_encoder_2.text_model.encoder.layers.31.mlp.fc1 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.31.mlp.fc2 :: Linear [CROSS]
text_encoder_2.text_model.encoder.layers.31.layer_norm2 :: LayerNorm [CROSS]
text_encoder_2.text_model.final_layer_norm :: LayerNorm [CROSS]
text_encoder_2.text_projection :: Linear [CROSS]
text_encoder_3 :: T5EncoderModel [CROSS]
text_encoder_3.shared :: Embedding [CROSS]
text_encoder_3.encoder :: T5Stack [CROSS]
text_encoder_3.encoder.block :: ModuleList [CROSS]
text_encoder_3.encoder.block.0 :: T5Block [CROSS]
text_encoder_3.encoder.block.0.layer :: ModuleList [CROSS]
text_encoder_3.encoder.block.0.layer.0 :: T5LayerSelfAttention [CROSS]
text_encoder_3.encoder.block.0.layer.0.SelfAttention :: T5Attention [CROSS]
text_encoder_3.encoder.block.0.layer.0.SelfAttention.q :: Linear [CROSS]
text_encoder_3.encoder.block.0.layer.0.SelfAttention.k :: Linear [CROSS]
text_encoder_3.encoder.block.0.layer.0.SelfAttention.v :: Linear [CROSS]
text_encoder_3.encoder.block.0.layer.0.SelfAttention.o :: Linear [CROSS]
text_encoder_3.encoder.block.0.layer.0.SelfAttention.relative_attention_bias :: Embedding [CROSS]
text_encoder_3.encoder.block.0.layer.0.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.0.layer.0.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.0.layer.1 :: T5LayerFF [CROSS]
text_encoder_3.encoder.block.0.layer.1.DenseReluDense :: T5DenseGatedActDense [CROSS]
text_encoder_3.encoder.block.0.layer.1.DenseReluDense.wi_0 :: Linear [CROSS]
text_encoder_3.encoder.block.0.layer.1.DenseReluDense.wi_1 :: Linear [CROSS]
text_encoder_3.encoder.block.0.layer.1.DenseReluDense.wo :: Linear [CROSS]
text_encoder_3.encoder.block.0.layer.1.DenseReluDense.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.0.layer.1.DenseReluDense.act :: NewGELUActivation [CROSS]
text_encoder_3.encoder.block.0.layer.1.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.0.layer.1.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.1 :: T5Block [CROSS]
text_encoder_3.encoder.block.1.layer :: ModuleList [CROSS]
text_encoder_3.encoder.block.1.layer.0 :: T5LayerSelfAttention [CROSS]
text_encoder_3.encoder.block.1.layer.0.SelfAttention :: T5Attention [CROSS]
text_encoder_3.encoder.block.1.layer.0.SelfAttention.q :: Linear [CROSS]
text_encoder_3.encoder.block.1.layer.0.SelfAttention.k :: Linear [CROSS]
text_encoder_3.encoder.block.1.layer.0.SelfAttention.v :: Linear [CROSS]
text_encoder_3.encoder.block.1.layer.0.SelfAttention.o :: Linear [CROSS]
text_encoder_3.encoder.block.1.layer.0.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.1.layer.0.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.1.layer.1 :: T5LayerFF [CROSS]
text_encoder_3.encoder.block.1.layer.1.DenseReluDense :: T5DenseGatedActDense [CROSS]
text_encoder_3.encoder.block.1.layer.1.DenseReluDense.wi_0 :: Linear [CROSS]
text_encoder_3.encoder.block.1.layer.1.DenseReluDense.wi_1 :: Linear [CROSS]
text_encoder_3.encoder.block.1.layer.1.DenseReluDense.wo :: Linear [CROSS]
text_encoder_3.encoder.block.1.layer.1.DenseReluDense.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.1.layer.1.DenseReluDense.act :: NewGELUActivation [CROSS]
text_encoder_3.encoder.block.1.layer.1.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.1.layer.1.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.2 :: T5Block [CROSS]
text_encoder_3.encoder.block.2.layer :: ModuleList [CROSS]
text_encoder_3.encoder.block.2.layer.0 :: T5LayerSelfAttention [CROSS]
text_encoder_3.encoder.block.2.layer.0.SelfAttention :: T5Attention [CROSS]
text_encoder_3.encoder.block.2.layer.0.SelfAttention.q :: Linear [CROSS]
text_encoder_3.encoder.block.2.layer.0.SelfAttention.k :: Linear [CROSS]
text_encoder_3.encoder.block.2.layer.0.SelfAttention.v :: Linear [CROSS]
text_encoder_3.encoder.block.2.layer.0.SelfAttention.o :: Linear [CROSS]
text_encoder_3.encoder.block.2.layer.0.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.2.layer.0.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.2.layer.1 :: T5LayerFF [CROSS]
text_encoder_3.encoder.block.2.layer.1.DenseReluDense :: T5DenseGatedActDense [CROSS]
text_encoder_3.encoder.block.2.layer.1.DenseReluDense.wi_0 :: Linear [CROSS]
text_encoder_3.encoder.block.2.layer.1.DenseReluDense.wi_1 :: Linear [CROSS]
text_encoder_3.encoder.block.2.layer.1.DenseReluDense.wo :: Linear [CROSS]
text_encoder_3.encoder.block.2.layer.1.DenseReluDense.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.2.layer.1.DenseReluDense.act :: NewGELUActivation [CROSS]
text_encoder_3.encoder.block.2.layer.1.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.2.layer.1.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.3 :: T5Block [CROSS]
text_encoder_3.encoder.block.3.layer :: ModuleList [CROSS]
text_encoder_3.encoder.block.3.layer.0 :: T5LayerSelfAttention [CROSS]
text_encoder_3.encoder.block.3.layer.0.SelfAttention :: T5Attention [CROSS]
text_encoder_3.encoder.block.3.layer.0.SelfAttention.q :: Linear [CROSS]
text_encoder_3.encoder.block.3.layer.0.SelfAttention.k :: Linear [CROSS]
text_encoder_3.encoder.block.3.layer.0.SelfAttention.v :: Linear [CROSS]
text_encoder_3.encoder.block.3.layer.0.SelfAttention.o :: Linear [CROSS]
text_encoder_3.encoder.block.3.layer.0.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.3.layer.0.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.3.layer.1 :: T5LayerFF [CROSS]
text_encoder_3.encoder.block.3.layer.1.DenseReluDense :: T5DenseGatedActDense [CROSS]
text_encoder_3.encoder.block.3.layer.1.DenseReluDense.wi_0 :: Linear [CROSS]
text_encoder_3.encoder.block.3.layer.1.DenseReluDense.wi_1 :: Linear [CROSS]
text_encoder_3.encoder.block.3.layer.1.DenseReluDense.wo :: Linear [CROSS]
text_encoder_3.encoder.block.3.layer.1.DenseReluDense.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.3.layer.1.DenseReluDense.act :: NewGELUActivation [CROSS]
text_encoder_3.encoder.block.3.layer.1.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.3.layer.1.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.4 :: T5Block [CROSS]
text_encoder_3.encoder.block.4.layer :: ModuleList [CROSS]
text_encoder_3.encoder.block.4.layer.0 :: T5LayerSelfAttention [CROSS]
text_encoder_3.encoder.block.4.layer.0.SelfAttention :: T5Attention [CROSS]
text_encoder_3.encoder.block.4.layer.0.SelfAttention.q :: Linear [CROSS]
text_encoder_3.encoder.block.4.layer.0.SelfAttention.k :: Linear [CROSS]
text_encoder_3.encoder.block.4.layer.0.SelfAttention.v :: Linear [CROSS]
text_encoder_3.encoder.block.4.layer.0.SelfAttention.o :: Linear [CROSS]
text_encoder_3.encoder.block.4.layer.0.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.4.layer.0.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.4.layer.1 :: T5LayerFF [CROSS]
text_encoder_3.encoder.block.4.layer.1.DenseReluDense :: T5DenseGatedActDense [CROSS]
text_encoder_3.encoder.block.4.layer.1.DenseReluDense.wi_0 :: Linear [CROSS]
text_encoder_3.encoder.block.4.layer.1.DenseReluDense.wi_1 :: Linear [CROSS]
text_encoder_3.encoder.block.4.layer.1.DenseReluDense.wo :: Linear [CROSS]
text_encoder_3.encoder.block.4.layer.1.DenseReluDense.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.4.layer.1.DenseReluDense.act :: NewGELUActivation [CROSS]
text_encoder_3.encoder.block.4.layer.1.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.4.layer.1.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.5 :: T5Block [CROSS]
text_encoder_3.encoder.block.5.layer :: ModuleList [CROSS]
text_encoder_3.encoder.block.5.layer.0 :: T5LayerSelfAttention [CROSS]
text_encoder_3.encoder.block.5.layer.0.SelfAttention :: T5Attention [CROSS]
text_encoder_3.encoder.block.5.layer.0.SelfAttention.q :: Linear [CROSS]
text_encoder_3.encoder.block.5.layer.0.SelfAttention.k :: Linear [CROSS]
text_encoder_3.encoder.block.5.layer.0.SelfAttention.v :: Linear [CROSS]
text_encoder_3.encoder.block.5.layer.0.SelfAttention.o :: Linear [CROSS]
text_encoder_3.encoder.block.5.layer.0.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.5.layer.0.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.5.layer.1 :: T5LayerFF [CROSS]
text_encoder_3.encoder.block.5.layer.1.DenseReluDense :: T5DenseGatedActDense [CROSS]
text_encoder_3.encoder.block.5.layer.1.DenseReluDense.wi_0 :: Linear [CROSS]
text_encoder_3.encoder.block.5.layer.1.DenseReluDense.wi_1 :: Linear [CROSS]
text_encoder_3.encoder.block.5.layer.1.DenseReluDense.wo :: Linear [CROSS]
text_encoder_3.encoder.block.5.layer.1.DenseReluDense.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.5.layer.1.DenseReluDense.act :: NewGELUActivation [CROSS]
text_encoder_3.encoder.block.5.layer.1.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.5.layer.1.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.6 :: T5Block [CROSS]
text_encoder_3.encoder.block.6.layer :: ModuleList [CROSS]
text_encoder_3.encoder.block.6.layer.0 :: T5LayerSelfAttention [CROSS]
text_encoder_3.encoder.block.6.layer.0.SelfAttention :: T5Attention [CROSS]
text_encoder_3.encoder.block.6.layer.0.SelfAttention.q :: Linear [CROSS]
text_encoder_3.encoder.block.6.layer.0.SelfAttention.k :: Linear [CROSS]
text_encoder_3.encoder.block.6.layer.0.SelfAttention.v :: Linear [CROSS]
text_encoder_3.encoder.block.6.layer.0.SelfAttention.o :: Linear [CROSS]
text_encoder_3.encoder.block.6.layer.0.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.6.layer.0.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.6.layer.1 :: T5LayerFF [CROSS]
text_encoder_3.encoder.block.6.layer.1.DenseReluDense :: T5DenseGatedActDense [CROSS]
text_encoder_3.encoder.block.6.layer.1.DenseReluDense.wi_0 :: Linear [CROSS]
text_encoder_3.encoder.block.6.layer.1.DenseReluDense.wi_1 :: Linear [CROSS]
text_encoder_3.encoder.block.6.layer.1.DenseReluDense.wo :: Linear [CROSS]
text_encoder_3.encoder.block.6.layer.1.DenseReluDense.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.6.layer.1.DenseReluDense.act :: NewGELUActivation [CROSS]
text_encoder_3.encoder.block.6.layer.1.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.6.layer.1.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.7 :: T5Block [CROSS]
text_encoder_3.encoder.block.7.layer :: ModuleList [CROSS]
text_encoder_3.encoder.block.7.layer.0 :: T5LayerSelfAttention [CROSS]
text_encoder_3.encoder.block.7.layer.0.SelfAttention :: T5Attention [CROSS]
text_encoder_3.encoder.block.7.layer.0.SelfAttention.q :: Linear [CROSS]
text_encoder_3.encoder.block.7.layer.0.SelfAttention.k :: Linear [CROSS]
text_encoder_3.encoder.block.7.layer.0.SelfAttention.v :: Linear [CROSS]
text_encoder_3.encoder.block.7.layer.0.SelfAttention.o :: Linear [CROSS]
text_encoder_3.encoder.block.7.layer.0.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.7.layer.0.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.7.layer.1 :: T5LayerFF [CROSS]
text_encoder_3.encoder.block.7.layer.1.DenseReluDense :: T5DenseGatedActDense [CROSS]
text_encoder_3.encoder.block.7.layer.1.DenseReluDense.wi_0 :: Linear [CROSS]
text_encoder_3.encoder.block.7.layer.1.DenseReluDense.wi_1 :: Linear [CROSS]
text_encoder_3.encoder.block.7.layer.1.DenseReluDense.wo :: Linear [CROSS]
text_encoder_3.encoder.block.7.layer.1.DenseReluDense.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.7.layer.1.DenseReluDense.act :: NewGELUActivation [CROSS]
text_encoder_3.encoder.block.7.layer.1.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.7.layer.1.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.8 :: T5Block [CROSS]
text_encoder_3.encoder.block.8.layer :: ModuleList [CROSS]
text_encoder_3.encoder.block.8.layer.0 :: T5LayerSelfAttention [CROSS]
text_encoder_3.encoder.block.8.layer.0.SelfAttention :: T5Attention [CROSS]
text_encoder_3.encoder.block.8.layer.0.SelfAttention.q :: Linear [CROSS]
text_encoder_3.encoder.block.8.layer.0.SelfAttention.k :: Linear [CROSS]
text_encoder_3.encoder.block.8.layer.0.SelfAttention.v :: Linear [CROSS]
text_encoder_3.encoder.block.8.layer.0.SelfAttention.o :: Linear [CROSS]
text_encoder_3.encoder.block.8.layer.0.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.8.layer.0.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.8.layer.1 :: T5LayerFF [CROSS]
text_encoder_3.encoder.block.8.layer.1.DenseReluDense :: T5DenseGatedActDense [CROSS]
text_encoder_3.encoder.block.8.layer.1.DenseReluDense.wi_0 :: Linear [CROSS]
text_encoder_3.encoder.block.8.layer.1.DenseReluDense.wi_1 :: Linear [CROSS]
text_encoder_3.encoder.block.8.layer.1.DenseReluDense.wo :: Linear [CROSS]
text_encoder_3.encoder.block.8.layer.1.DenseReluDense.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.8.layer.1.DenseReluDense.act :: NewGELUActivation [CROSS]
text_encoder_3.encoder.block.8.layer.1.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.8.layer.1.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.9 :: T5Block [CROSS]
text_encoder_3.encoder.block.9.layer :: ModuleList [CROSS]
text_encoder_3.encoder.block.9.layer.0 :: T5LayerSelfAttention [CROSS]
text_encoder_3.encoder.block.9.layer.0.SelfAttention :: T5Attention [CROSS]
text_encoder_3.encoder.block.9.layer.0.SelfAttention.q :: Linear [CROSS]
text_encoder_3.encoder.block.9.layer.0.SelfAttention.k :: Linear [CROSS]
text_encoder_3.encoder.block.9.layer.0.SelfAttention.v :: Linear [CROSS]
text_encoder_3.encoder.block.9.layer.0.SelfAttention.o :: Linear [CROSS]
text_encoder_3.encoder.block.9.layer.0.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.9.layer.0.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.9.layer.1 :: T5LayerFF [CROSS]
text_encoder_3.encoder.block.9.layer.1.DenseReluDense :: T5DenseGatedActDense [CROSS]
text_encoder_3.encoder.block.9.layer.1.DenseReluDense.wi_0 :: Linear [CROSS]
text_encoder_3.encoder.block.9.layer.1.DenseReluDense.wi_1 :: Linear [CROSS]
text_encoder_3.encoder.block.9.layer.1.DenseReluDense.wo :: Linear [CROSS]
text_encoder_3.encoder.block.9.layer.1.DenseReluDense.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.9.layer.1.DenseReluDense.act :: NewGELUActivation [CROSS]
text_encoder_3.encoder.block.9.layer.1.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.9.layer.1.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.10 :: T5Block [CROSS]
text_encoder_3.encoder.block.10.layer :: ModuleList [CROSS]
text_encoder_3.encoder.block.10.layer.0 :: T5LayerSelfAttention [CROSS]
text_encoder_3.encoder.block.10.layer.0.SelfAttention :: T5Attention [CROSS]
text_encoder_3.encoder.block.10.layer.0.SelfAttention.q :: Linear [CROSS]
text_encoder_3.encoder.block.10.layer.0.SelfAttention.k :: Linear [CROSS]
text_encoder_3.encoder.block.10.layer.0.SelfAttention.v :: Linear [CROSS]
text_encoder_3.encoder.block.10.layer.0.SelfAttention.o :: Linear [CROSS]
text_encoder_3.encoder.block.10.layer.0.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.10.layer.0.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.10.layer.1 :: T5LayerFF [CROSS]
text_encoder_3.encoder.block.10.layer.1.DenseReluDense :: T5DenseGatedActDense [CROSS]
text_encoder_3.encoder.block.10.layer.1.DenseReluDense.wi_0 :: Linear [CROSS]
text_encoder_3.encoder.block.10.layer.1.DenseReluDense.wi_1 :: Linear [CROSS]
text_encoder_3.encoder.block.10.layer.1.DenseReluDense.wo :: Linear [CROSS]
text_encoder_3.encoder.block.10.layer.1.DenseReluDense.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.10.layer.1.DenseReluDense.act :: NewGELUActivation [CROSS]
text_encoder_3.encoder.block.10.layer.1.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.10.layer.1.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.11 :: T5Block [CROSS]
text_encoder_3.encoder.block.11.layer :: ModuleList [CROSS]
text_encoder_3.encoder.block.11.layer.0 :: T5LayerSelfAttention [CROSS]
text_encoder_3.encoder.block.11.layer.0.SelfAttention :: T5Attention [CROSS]
text_encoder_3.encoder.block.11.layer.0.SelfAttention.q :: Linear [CROSS]
text_encoder_3.encoder.block.11.layer.0.SelfAttention.k :: Linear [CROSS]
text_encoder_3.encoder.block.11.layer.0.SelfAttention.v :: Linear [CROSS]
text_encoder_3.encoder.block.11.layer.0.SelfAttention.o :: Linear [CROSS]
text_encoder_3.encoder.block.11.layer.0.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.11.layer.0.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.11.layer.1 :: T5LayerFF [CROSS]
text_encoder_3.encoder.block.11.layer.1.DenseReluDense :: T5DenseGatedActDense [CROSS]
text_encoder_3.encoder.block.11.layer.1.DenseReluDense.wi_0 :: Linear [CROSS]
text_encoder_3.encoder.block.11.layer.1.DenseReluDense.wi_1 :: Linear [CROSS]
text_encoder_3.encoder.block.11.layer.1.DenseReluDense.wo :: Linear [CROSS]
text_encoder_3.encoder.block.11.layer.1.DenseReluDense.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.11.layer.1.DenseReluDense.act :: NewGELUActivation [CROSS]
text_encoder_3.encoder.block.11.layer.1.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.11.layer.1.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.12 :: T5Block [CROSS]
text_encoder_3.encoder.block.12.layer :: ModuleList [CROSS]
text_encoder_3.encoder.block.12.layer.0 :: T5LayerSelfAttention [CROSS]
text_encoder_3.encoder.block.12.layer.0.SelfAttention :: T5Attention [CROSS]
text_encoder_3.encoder.block.12.layer.0.SelfAttention.q :: Linear [CROSS]
text_encoder_3.encoder.block.12.layer.0.SelfAttention.k :: Linear [CROSS]
text_encoder_3.encoder.block.12.layer.0.SelfAttention.v :: Linear [CROSS]
text_encoder_3.encoder.block.12.layer.0.SelfAttention.o :: Linear [CROSS]
text_encoder_3.encoder.block.12.layer.0.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.12.layer.0.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.12.layer.1 :: T5LayerFF [CROSS]
text_encoder_3.encoder.block.12.layer.1.DenseReluDense :: T5DenseGatedActDense [CROSS]
text_encoder_3.encoder.block.12.layer.1.DenseReluDense.wi_0 :: Linear [CROSS]
text_encoder_3.encoder.block.12.layer.1.DenseReluDense.wi_1 :: Linear [CROSS]
text_encoder_3.encoder.block.12.layer.1.DenseReluDense.wo :: Linear [CROSS]
text_encoder_3.encoder.block.12.layer.1.DenseReluDense.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.12.layer.1.DenseReluDense.act :: NewGELUActivation [CROSS]
text_encoder_3.encoder.block.12.layer.1.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.12.layer.1.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.13 :: T5Block [CROSS]
text_encoder_3.encoder.block.13.layer :: ModuleList [CROSS]
text_encoder_3.encoder.block.13.layer.0 :: T5LayerSelfAttention [CROSS]
text_encoder_3.encoder.block.13.layer.0.SelfAttention :: T5Attention [CROSS]
text_encoder_3.encoder.block.13.layer.0.SelfAttention.q :: Linear [CROSS]
text_encoder_3.encoder.block.13.layer.0.SelfAttention.k :: Linear [CROSS]
text_encoder_3.encoder.block.13.layer.0.SelfAttention.v :: Linear [CROSS]
text_encoder_3.encoder.block.13.layer.0.SelfAttention.o :: Linear [CROSS]
text_encoder_3.encoder.block.13.layer.0.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.13.layer.0.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.13.layer.1 :: T5LayerFF [CROSS]
text_encoder_3.encoder.block.13.layer.1.DenseReluDense :: T5DenseGatedActDense [CROSS]
text_encoder_3.encoder.block.13.layer.1.DenseReluDense.wi_0 :: Linear [CROSS]
text_encoder_3.encoder.block.13.layer.1.DenseReluDense.wi_1 :: Linear [CROSS]
text_encoder_3.encoder.block.13.layer.1.DenseReluDense.wo :: Linear [CROSS]
text_encoder_3.encoder.block.13.layer.1.DenseReluDense.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.13.layer.1.DenseReluDense.act :: NewGELUActivation [CROSS]
text_encoder_3.encoder.block.13.layer.1.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.13.layer.1.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.14 :: T5Block [CROSS]
text_encoder_3.encoder.block.14.layer :: ModuleList [CROSS]
text_encoder_3.encoder.block.14.layer.0 :: T5LayerSelfAttention [CROSS]
text_encoder_3.encoder.block.14.layer.0.SelfAttention :: T5Attention [CROSS]
text_encoder_3.encoder.block.14.layer.0.SelfAttention.q :: Linear [CROSS]
text_encoder_3.encoder.block.14.layer.0.SelfAttention.k :: Linear [CROSS]
text_encoder_3.encoder.block.14.layer.0.SelfAttention.v :: Linear [CROSS]
text_encoder_3.encoder.block.14.layer.0.SelfAttention.o :: Linear [CROSS]
text_encoder_3.encoder.block.14.layer.0.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.14.layer.0.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.14.layer.1 :: T5LayerFF [CROSS]
text_encoder_3.encoder.block.14.layer.1.DenseReluDense :: T5DenseGatedActDense [CROSS]
text_encoder_3.encoder.block.14.layer.1.DenseReluDense.wi_0 :: Linear [CROSS]
text_encoder_3.encoder.block.14.layer.1.DenseReluDense.wi_1 :: Linear [CROSS]
text_encoder_3.encoder.block.14.layer.1.DenseReluDense.wo :: Linear [CROSS]
text_encoder_3.encoder.block.14.layer.1.DenseReluDense.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.14.layer.1.DenseReluDense.act :: NewGELUActivation [CROSS]
text_encoder_3.encoder.block.14.layer.1.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.14.layer.1.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.15 :: T5Block [CROSS]
text_encoder_3.encoder.block.15.layer :: ModuleList [CROSS]
text_encoder_3.encoder.block.15.layer.0 :: T5LayerSelfAttention [CROSS]
text_encoder_3.encoder.block.15.layer.0.SelfAttention :: T5Attention [CROSS]
text_encoder_3.encoder.block.15.layer.0.SelfAttention.q :: Linear [CROSS]
text_encoder_3.encoder.block.15.layer.0.SelfAttention.k :: Linear [CROSS]
text_encoder_3.encoder.block.15.layer.0.SelfAttention.v :: Linear [CROSS]
text_encoder_3.encoder.block.15.layer.0.SelfAttention.o :: Linear [CROSS]
text_encoder_3.encoder.block.15.layer.0.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.15.layer.0.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.15.layer.1 :: T5LayerFF [CROSS]
text_encoder_3.encoder.block.15.layer.1.DenseReluDense :: T5DenseGatedActDense [CROSS]
text_encoder_3.encoder.block.15.layer.1.DenseReluDense.wi_0 :: Linear [CROSS]
text_encoder_3.encoder.block.15.layer.1.DenseReluDense.wi_1 :: Linear [CROSS]
text_encoder_3.encoder.block.15.layer.1.DenseReluDense.wo :: Linear [CROSS]
text_encoder_3.encoder.block.15.layer.1.DenseReluDense.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.15.layer.1.DenseReluDense.act :: NewGELUActivation [CROSS]
text_encoder_3.encoder.block.15.layer.1.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.15.layer.1.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.16 :: T5Block [CROSS]
text_encoder_3.encoder.block.16.layer :: ModuleList [CROSS]
text_encoder_3.encoder.block.16.layer.0 :: T5LayerSelfAttention [CROSS]
text_encoder_3.encoder.block.16.layer.0.SelfAttention :: T5Attention [CROSS]
text_encoder_3.encoder.block.16.layer.0.SelfAttention.q :: Linear [CROSS]
text_encoder_3.encoder.block.16.layer.0.SelfAttention.k :: Linear [CROSS]
text_encoder_3.encoder.block.16.layer.0.SelfAttention.v :: Linear [CROSS]
text_encoder_3.encoder.block.16.layer.0.SelfAttention.o :: Linear [CROSS]
text_encoder_3.encoder.block.16.layer.0.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.16.layer.0.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.16.layer.1 :: T5LayerFF [CROSS]
text_encoder_3.encoder.block.16.layer.1.DenseReluDense :: T5DenseGatedActDense [CROSS]
text_encoder_3.encoder.block.16.layer.1.DenseReluDense.wi_0 :: Linear [CROSS]
text_encoder_3.encoder.block.16.layer.1.DenseReluDense.wi_1 :: Linear [CROSS]
text_encoder_3.encoder.block.16.layer.1.DenseReluDense.wo :: Linear [CROSS]
text_encoder_3.encoder.block.16.layer.1.DenseReluDense.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.16.layer.1.DenseReluDense.act :: NewGELUActivation [CROSS]
text_encoder_3.encoder.block.16.layer.1.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.16.layer.1.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.17 :: T5Block [CROSS]
text_encoder_3.encoder.block.17.layer :: ModuleList [CROSS]
text_encoder_3.encoder.block.17.layer.0 :: T5LayerSelfAttention [CROSS]
text_encoder_3.encoder.block.17.layer.0.SelfAttention :: T5Attention [CROSS]
text_encoder_3.encoder.block.17.layer.0.SelfAttention.q :: Linear [CROSS]
text_encoder_3.encoder.block.17.layer.0.SelfAttention.k :: Linear [CROSS]
text_encoder_3.encoder.block.17.layer.0.SelfAttention.v :: Linear [CROSS]
text_encoder_3.encoder.block.17.layer.0.SelfAttention.o :: Linear [CROSS]
text_encoder_3.encoder.block.17.layer.0.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.17.layer.0.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.17.layer.1 :: T5LayerFF [CROSS]
text_encoder_3.encoder.block.17.layer.1.DenseReluDense :: T5DenseGatedActDense [CROSS]
text_encoder_3.encoder.block.17.layer.1.DenseReluDense.wi_0 :: Linear [CROSS]
text_encoder_3.encoder.block.17.layer.1.DenseReluDense.wi_1 :: Linear [CROSS]
text_encoder_3.encoder.block.17.layer.1.DenseReluDense.wo :: Linear [CROSS]
text_encoder_3.encoder.block.17.layer.1.DenseReluDense.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.17.layer.1.DenseReluDense.act :: NewGELUActivation [CROSS]
text_encoder_3.encoder.block.17.layer.1.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.17.layer.1.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.18 :: T5Block [CROSS]
text_encoder_3.encoder.block.18.layer :: ModuleList [CROSS]
text_encoder_3.encoder.block.18.layer.0 :: T5LayerSelfAttention [CROSS]
text_encoder_3.encoder.block.18.layer.0.SelfAttention :: T5Attention [CROSS]
text_encoder_3.encoder.block.18.layer.0.SelfAttention.q :: Linear [CROSS]
text_encoder_3.encoder.block.18.layer.0.SelfAttention.k :: Linear [CROSS]
text_encoder_3.encoder.block.18.layer.0.SelfAttention.v :: Linear [CROSS]
text_encoder_3.encoder.block.18.layer.0.SelfAttention.o :: Linear [CROSS]
text_encoder_3.encoder.block.18.layer.0.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.18.layer.0.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.18.layer.1 :: T5LayerFF [CROSS]
text_encoder_3.encoder.block.18.layer.1.DenseReluDense :: T5DenseGatedActDense [CROSS]
text_encoder_3.encoder.block.18.layer.1.DenseReluDense.wi_0 :: Linear [CROSS]
text_encoder_3.encoder.block.18.layer.1.DenseReluDense.wi_1 :: Linear [CROSS]
text_encoder_3.encoder.block.18.layer.1.DenseReluDense.wo :: Linear [CROSS]
text_encoder_3.encoder.block.18.layer.1.DenseReluDense.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.18.layer.1.DenseReluDense.act :: NewGELUActivation [CROSS]
text_encoder_3.encoder.block.18.layer.1.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.18.layer.1.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.19 :: T5Block [CROSS]
text_encoder_3.encoder.block.19.layer :: ModuleList [CROSS]
text_encoder_3.encoder.block.19.layer.0 :: T5LayerSelfAttention [CROSS]
text_encoder_3.encoder.block.19.layer.0.SelfAttention :: T5Attention [CROSS]
text_encoder_3.encoder.block.19.layer.0.SelfAttention.q :: Linear [CROSS]
text_encoder_3.encoder.block.19.layer.0.SelfAttention.k :: Linear [CROSS]
text_encoder_3.encoder.block.19.layer.0.SelfAttention.v :: Linear [CROSS]
text_encoder_3.encoder.block.19.layer.0.SelfAttention.o :: Linear [CROSS]
text_encoder_3.encoder.block.19.layer.0.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.19.layer.0.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.19.layer.1 :: T5LayerFF [CROSS]
text_encoder_3.encoder.block.19.layer.1.DenseReluDense :: T5DenseGatedActDense [CROSS]
text_encoder_3.encoder.block.19.layer.1.DenseReluDense.wi_0 :: Linear [CROSS]
text_encoder_3.encoder.block.19.layer.1.DenseReluDense.wi_1 :: Linear [CROSS]
text_encoder_3.encoder.block.19.layer.1.DenseReluDense.wo :: Linear [CROSS]
text_encoder_3.encoder.block.19.layer.1.DenseReluDense.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.19.layer.1.DenseReluDense.act :: NewGELUActivation [CROSS]
text_encoder_3.encoder.block.19.layer.1.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.19.layer.1.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.20 :: T5Block [CROSS]
text_encoder_3.encoder.block.20.layer :: ModuleList [CROSS]
text_encoder_3.encoder.block.20.layer.0 :: T5LayerSelfAttention [CROSS]
text_encoder_3.encoder.block.20.layer.0.SelfAttention :: T5Attention [CROSS]
text_encoder_3.encoder.block.20.layer.0.SelfAttention.q :: Linear [CROSS]
text_encoder_3.encoder.block.20.layer.0.SelfAttention.k :: Linear [CROSS]
text_encoder_3.encoder.block.20.layer.0.SelfAttention.v :: Linear [CROSS]
text_encoder_3.encoder.block.20.layer.0.SelfAttention.o :: Linear [CROSS]
text_encoder_3.encoder.block.20.layer.0.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.20.layer.0.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.20.layer.1 :: T5LayerFF [CROSS]
text_encoder_3.encoder.block.20.layer.1.DenseReluDense :: T5DenseGatedActDense [CROSS]
text_encoder_3.encoder.block.20.layer.1.DenseReluDense.wi_0 :: Linear [CROSS]
text_encoder_3.encoder.block.20.layer.1.DenseReluDense.wi_1 :: Linear [CROSS]
text_encoder_3.encoder.block.20.layer.1.DenseReluDense.wo :: Linear [CROSS]
text_encoder_3.encoder.block.20.layer.1.DenseReluDense.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.20.layer.1.DenseReluDense.act :: NewGELUActivation [CROSS]
text_encoder_3.encoder.block.20.layer.1.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.20.layer.1.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.21 :: T5Block [CROSS]
text_encoder_3.encoder.block.21.layer :: ModuleList [CROSS]
text_encoder_3.encoder.block.21.layer.0 :: T5LayerSelfAttention [CROSS]
text_encoder_3.encoder.block.21.layer.0.SelfAttention :: T5Attention [CROSS]
text_encoder_3.encoder.block.21.layer.0.SelfAttention.q :: Linear [CROSS]
text_encoder_3.encoder.block.21.layer.0.SelfAttention.k :: Linear [CROSS]
text_encoder_3.encoder.block.21.layer.0.SelfAttention.v :: Linear [CROSS]
text_encoder_3.encoder.block.21.layer.0.SelfAttention.o :: Linear [CROSS]
text_encoder_3.encoder.block.21.layer.0.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.21.layer.0.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.21.layer.1 :: T5LayerFF [CROSS]
text_encoder_3.encoder.block.21.layer.1.DenseReluDense :: T5DenseGatedActDense [CROSS]
text_encoder_3.encoder.block.21.layer.1.DenseReluDense.wi_0 :: Linear [CROSS]
text_encoder_3.encoder.block.21.layer.1.DenseReluDense.wi_1 :: Linear [CROSS]
text_encoder_3.encoder.block.21.layer.1.DenseReluDense.wo :: Linear [CROSS]
text_encoder_3.encoder.block.21.layer.1.DenseReluDense.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.21.layer.1.DenseReluDense.act :: NewGELUActivation [CROSS]
text_encoder_3.encoder.block.21.layer.1.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.21.layer.1.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.22 :: T5Block [CROSS]
text_encoder_3.encoder.block.22.layer :: ModuleList [CROSS]
text_encoder_3.encoder.block.22.layer.0 :: T5LayerSelfAttention [CROSS]
text_encoder_3.encoder.block.22.layer.0.SelfAttention :: T5Attention [CROSS]
text_encoder_3.encoder.block.22.layer.0.SelfAttention.q :: Linear [CROSS]
text_encoder_3.encoder.block.22.layer.0.SelfAttention.k :: Linear [CROSS]
text_encoder_3.encoder.block.22.layer.0.SelfAttention.v :: Linear [CROSS]
text_encoder_3.encoder.block.22.layer.0.SelfAttention.o :: Linear [CROSS]
text_encoder_3.encoder.block.22.layer.0.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.22.layer.0.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.22.layer.1 :: T5LayerFF [CROSS]
text_encoder_3.encoder.block.22.layer.1.DenseReluDense :: T5DenseGatedActDense [CROSS]
text_encoder_3.encoder.block.22.layer.1.DenseReluDense.wi_0 :: Linear [CROSS]
text_encoder_3.encoder.block.22.layer.1.DenseReluDense.wi_1 :: Linear [CROSS]
text_encoder_3.encoder.block.22.layer.1.DenseReluDense.wo :: Linear [CROSS]
text_encoder_3.encoder.block.22.layer.1.DenseReluDense.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.22.layer.1.DenseReluDense.act :: NewGELUActivation [CROSS]
text_encoder_3.encoder.block.22.layer.1.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.22.layer.1.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.23 :: T5Block [CROSS]
text_encoder_3.encoder.block.23.layer :: ModuleList [CROSS]
text_encoder_3.encoder.block.23.layer.0 :: T5LayerSelfAttention [CROSS]
text_encoder_3.encoder.block.23.layer.0.SelfAttention :: T5Attention [CROSS]
text_encoder_3.encoder.block.23.layer.0.SelfAttention.q :: Linear [CROSS]
text_encoder_3.encoder.block.23.layer.0.SelfAttention.k :: Linear [CROSS]
text_encoder_3.encoder.block.23.layer.0.SelfAttention.v :: Linear [CROSS]
text_encoder_3.encoder.block.23.layer.0.SelfAttention.o :: Linear [CROSS]
text_encoder_3.encoder.block.23.layer.0.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.23.layer.0.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.23.layer.1 :: T5LayerFF [CROSS]
text_encoder_3.encoder.block.23.layer.1.DenseReluDense :: T5DenseGatedActDense [CROSS]
text_encoder_3.encoder.block.23.layer.1.DenseReluDense.wi_0 :: Linear [CROSS]
text_encoder_3.encoder.block.23.layer.1.DenseReluDense.wi_1 :: Linear [CROSS]
text_encoder_3.encoder.block.23.layer.1.DenseReluDense.wo :: Linear [CROSS]
text_encoder_3.encoder.block.23.layer.1.DenseReluDense.dropout :: Dropout [CROSS]
text_encoder_3.encoder.block.23.layer.1.DenseReluDense.act :: NewGELUActivation [CROSS]
text_encoder_3.encoder.block.23.layer.1.layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.block.23.layer.1.dropout :: Dropout [CROSS]
text_encoder_3.encoder.final_layer_norm :: T5LayerNorm [CROSS]
text_encoder_3.encoder.dropout :: Dropout [CROSS]

=== SUMMARY ===
total scanned modules: 2291
QKV modules:          39
CROSS-labeled modules:1218
AdaLN-like (name):    0  (이건 이름 기반 휴리스틱)

=== SIMULATE: freeze_all_but_self_attn_qkv ===
include_adaln=False
transformer params total=2243.17M | would-be-trainable=349.40M
would keep 296 parameter tensors (show up to 40):
  - transformer_blocks.0.attn.to_q.weight
  - transformer_blocks.0.attn.to_q.bias
  - transformer_blocks.0.attn.to_k.weight
  - transformer_blocks.0.attn.to_k.bias
  - transformer_blocks.0.attn.to_v.weight
  - transformer_blocks.0.attn.to_v.bias
  - transformer_blocks.0.attn.to_out.0.weight
  - transformer_blocks.0.attn.to_out.0.bias
  - transformer_blocks.0.attn2.to_q.weight
  - transformer_blocks.0.attn2.to_q.bias
  - transformer_blocks.0.attn2.to_k.weight
  - transformer_blocks.0.attn2.to_k.bias
  - transformer_blocks.0.attn2.to_v.weight
  - transformer_blocks.0.attn2.to_v.bias
  - transformer_blocks.0.attn2.to_out.0.weight
  - transformer_blocks.0.attn2.to_out.0.bias
  - transformer_blocks.1.attn.to_q.weight
  - transformer_blocks.1.attn.to_q.bias
  - transformer_blocks.1.attn.to_k.weight
  - transformer_blocks.1.attn.to_k.bias
  - transformer_blocks.1.attn.to_v.weight
  - transformer_blocks.1.attn.to_v.bias
  - transformer_blocks.1.attn.to_out.0.weight
  - transformer_blocks.1.attn.to_out.0.bias
  - transformer_blocks.1.attn2.to_q.weight
  - transformer_blocks.1.attn2.to_q.bias
  - transformer_blocks.1.attn2.to_k.weight
  - transformer_blocks.1.attn2.to_k.bias
  - transformer_blocks.1.attn2.to_v.weight
  - transformer_blocks.1.attn2.to_v.bias
  - transformer_blocks.1.attn2.to_out.0.weight
  - transformer_blocks.1.attn2.to_out.0.bias
  - transformer_blocks.2.attn.to_q.weight
  - transformer_blocks.2.attn.to_q.bias
  - transformer_blocks.2.attn.to_k.weight
  - transformer_blocks.2.attn.to_k.bias
  - transformer_blocks.2.attn.to_v.weight
  - transformer_blocks.2.attn.to_v.bias
  - transformer_blocks.2.attn.to_out.0.weight
  - transformer_blocks.2.attn.to_out.0.bias
